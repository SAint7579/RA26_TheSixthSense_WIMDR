{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "garbage_segregation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8KTofTcDIFD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "2f9f2669-1876-4ab3-f4e8-a47e5e19d69a"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jun  4 20:07:44 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P8    32W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "69YZC_tMBknJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c36f9fb7-f853-4102-80da-45668cac9435"
      },
      "source": [
        "!apt-get install protobuf-compiler python-pil python-lxml python-tk\n",
        "!pip install Cython\n",
        "!pip install matplotlib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "protobuf-compiler is already the newest version (3.0.0-9.1ubuntu1).\n",
            "python-tk is already the newest version (2.7.17-1~18.04).\n",
            "The following additional packages will be installed:\n",
            "  python-bs4 python-chardet python-html5lib python-olefile\n",
            "  python-pkg-resources python-six python-webencodings\n",
            "Suggested packages:\n",
            "  python-genshi python-lxml-dbg python-lxml-doc python-pil-doc python-pil-dbg\n",
            "  python-setuptools\n",
            "The following NEW packages will be installed:\n",
            "  python-bs4 python-chardet python-html5lib python-lxml python-olefile\n",
            "  python-pil python-pkg-resources python-six python-webencodings\n",
            "0 upgraded, 9 newly installed, 0 to remove and 32 not upgraded.\n",
            "Need to get 1,818 kB of archives.\n",
            "After this operation, 7,685 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-bs4 all 4.6.0-1 [67.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-pkg-resources all 39.0.1-2 [128 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-chardet all 3.0.4-1 [80.3 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-six all 1.11.0-2 [11.3 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-webencodings all 0.5-2 [10.3 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-html5lib all 0.999999999-1 [83.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python-lxml amd64 4.2.1-1ubuntu0.1 [1,075 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-olefile all 0.45.1-1 [33.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python-pil amd64 5.1.0-1ubuntu0.2 [329 kB]\n",
            "Fetched 1,818 kB in 1s (1,706 kB/s)\n",
            "Selecting previously unselected package python-bs4.\n",
            "(Reading database ... 144467 files and directories currently installed.)\n",
            "Preparing to unpack .../0-python-bs4_4.6.0-1_all.deb ...\n",
            "Unpacking python-bs4 (4.6.0-1) ...\n",
            "Selecting previously unselected package python-pkg-resources.\n",
            "Preparing to unpack .../1-python-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python-chardet.\n",
            "Preparing to unpack .../2-python-chardet_3.0.4-1_all.deb ...\n",
            "Unpacking python-chardet (3.0.4-1) ...\n",
            "Selecting previously unselected package python-six.\n",
            "Preparing to unpack .../3-python-six_1.11.0-2_all.deb ...\n",
            "Unpacking python-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python-webencodings.\n",
            "Preparing to unpack .../4-python-webencodings_0.5-2_all.deb ...\n",
            "Unpacking python-webencodings (0.5-2) ...\n",
            "Selecting previously unselected package python-html5lib.\n",
            "Preparing to unpack .../5-python-html5lib_0.999999999-1_all.deb ...\n",
            "Unpacking python-html5lib (0.999999999-1) ...\n",
            "Selecting previously unselected package python-lxml:amd64.\n",
            "Preparing to unpack .../6-python-lxml_4.2.1-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python-olefile.\n",
            "Preparing to unpack .../7-python-olefile_0.45.1-1_all.deb ...\n",
            "Unpacking python-olefile (0.45.1-1) ...\n",
            "Selecting previously unselected package python-pil:amd64.\n",
            "Preparing to unpack .../8-python-pil_5.1.0-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking python-pil:amd64 (5.1.0-1ubuntu0.2) ...\n",
            "Setting up python-pkg-resources (39.0.1-2) ...\n",
            "Setting up python-six (1.11.0-2) ...\n",
            "Setting up python-bs4 (4.6.0-1) ...\n",
            "Setting up python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
            "Setting up python-olefile (0.45.1-1) ...\n",
            "Setting up python-pil:amd64 (5.1.0-1ubuntu0.2) ...\n",
            "Setting up python-webencodings (0.5-2) ...\n",
            "Setting up python-chardet (3.0.4-1) ...\n",
            "Setting up python-html5lib (0.999999999-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (0.29.19)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.18.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "50F8-y0eF2HK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "9576c389-9070-42d0-9e66-1d046b89d46a"
      },
      "source": [
        "!git clone https://github.com/tensorflow/models.git\n",
        "!pip install -q contextlib2\n",
        "!pip install -q pycocotools\n",
        "!pip install -q tf-slim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 36569 (delta 2), reused 7 (delta 0), pack-reused 36557\u001b[K\n",
            "Receiving objects: 100% (36569/36569), 520.39 MiB | 37.88 MiB/s, done.\n",
            "Resolving deltas: 100% (24406/24406), done.\n",
            "Collecting tf-slim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from tf-slim) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py>=0.2.2->tf-slim) (1.12.0)\n",
            "Installing collected packages: tf-slim\n",
            "Successfully installed tf-slim-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_KB2n158zC7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7df42861-38d7-400d-e174-006e1395a14d"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VZPf7e_yF_L7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9afbe6dd-797e-4819-e586-d7922fa62c9b"
      },
      "source": [
        "%cd /content/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'\n",
        "\n",
        "!python object_detection/builders/model_builder_test.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research\n",
            "object_detection/protos/input_reader.proto: warning: Import object_detection/protos/image_resizer.proto but not used.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a1Y1mFlRGUEJ",
        "colab": {}
      },
      "source": [
        "!mkdir /content/garbage_detection/\n",
        "!mv /content/generate_tfrecord.py /content/garbage_detection/\n",
        "!mv /content/label_map_cls.pbtxt /content/garbage_detection/label_map.pbtxt\n",
        "!mv /content/train_labels_cls.csv /content/garbage_detection/train_labels.csv\n",
        "!mv /content/test_labels_cls.csv /content/garbage_detection/test_labels.csv\n",
        "!mv /content/pipeline_cls.config /content/garbage_detection/pipeline.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Bd9Wcdd5gMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %cd /content\n",
        "# !git clone https://github.com/ShivamShrirao/TACO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZjFI7-ThSPZ5",
        "colab": {}
      },
      "source": [
        "# !mv /content/garbage_detection/data/garbage_dataset /content/TACO/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8JH_th6oveh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !rm -rf data/bat*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR6nkKZs6RNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %cd /content/TACO\n",
        "# !python download.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvE44GJQHBqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !zip -r data.zip data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ctmUn82Utx6F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "13d3f0ac-bcc1-4b7e-a924-23c948ebd05d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEHfNYVHJTy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !mv data.zip /content/drive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXk6MstDLeiv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/garbage_detection/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhy3AzomZNrA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/drive/My\\ Drive/data.zip /content/garbage_detection/data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L62U-_qQZhFn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2aa845f3-1318-4d00-f0ad-7633c2ef2462"
      },
      "source": [
        "%cd /content/garbage_detection/data/\n",
        "!unzip data.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/garbage_detection/data\n",
            "Archive:  data.zip\n",
            "   creating: garbage_dataset/\n",
            "   creating: garbage_dataset/batch_7/\n",
            "  inflating: garbage_dataset/batch_7/000107.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000068.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000127.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000054.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000097.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000124.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000133.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000122.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000113.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000087.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000128.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000092.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000089.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000016.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000057.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000094.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000126.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000063.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000117.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000093.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000075.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000090.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000029.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000004.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000064.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000114.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000110.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000019.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000125.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000006.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000104.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000018.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000012.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000071.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000058.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000109.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000049.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000065.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000079.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000005.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000044.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000034.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000008.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000123.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000108.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000047.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000010.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000045.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000100.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000076.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000098.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000002.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000142.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000086.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000138.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000120.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000083.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000017.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000000.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000085.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000070.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000024.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000096.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000037.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000022.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000069.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000015.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000081.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000140.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000003.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000139.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000129.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000084.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000088.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000020.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000033.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000056.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000111.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000119.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000036.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000080.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000091.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000023.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000042.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000025.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000043.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000055.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000062.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000039.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000053.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000066.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000078.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000030.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000077.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000095.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000001.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000132.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000021.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000038.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000131.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000141.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000052.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000102.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000011.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000067.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000031.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000121.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000112.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000082.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000101.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000134.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000115.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000014.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000073.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000051.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000035.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000136.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000013.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000048.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000072.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000137.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000135.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000118.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000106.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000060.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000103.JPG  \n",
            "  inflating: garbage_dataset/batch_7/000050.JPG  \n",
            "   creating: garbage_dataset/batch_14/\n",
            "  inflating: garbage_dataset/batch_14/000023.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000021.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000006.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000025.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000071.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000094.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000013.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000024.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000064.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000066.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000063.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000078.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000052.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000016.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000069.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000077.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000056.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000065.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000057.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000028.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000030.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000011.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000046.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000019.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000083.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000093.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000020.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000092.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000096.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000032.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000014.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000087.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000061.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000090.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000097.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000022.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000074.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000088.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000044.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000000.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000004.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000062.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000001.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000076.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000038.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000054.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000007.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000026.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000073.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000099.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000037.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000018.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000085.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000017.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000035.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000041.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000008.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000082.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000089.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000095.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000060.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000045.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000072.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000049.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000051.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000080.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000079.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000010.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000003.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000039.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000009.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000036.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000075.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000067.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000086.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000098.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000002.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000070.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000029.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000033.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000050.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000034.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000059.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000053.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000043.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000058.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000048.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000042.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000012.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000015.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000084.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000047.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000068.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000081.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000040.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000055.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000031.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000091.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000027.jpg  \n",
            "  inflating: garbage_dataset/batch_14/000005.jpg  \n",
            "   creating: garbage_dataset/batch_4/\n",
            "  inflating: garbage_dataset/batch_4/000068.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000007.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000054.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000097.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000087.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000092.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000089.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000016.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000057.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000094.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000063.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000093.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000090.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000029.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000004.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000064.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000019.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000006.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000040.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000018.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000012.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000071.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000058.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000049.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000065.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000079.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000005.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000034.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000008.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000026.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000027.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000047.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000010.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000045.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000041.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000076.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000009.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000098.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000032.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000002.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000086.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000083.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000000.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000085.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000070.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000096.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000037.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000074.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000022.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000069.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000015.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000081.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000003.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000084.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000088.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000020.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000056.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000061.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000036.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000080.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000023.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000042.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000025.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000043.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000055.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000062.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000039.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000053.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000059.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000066.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000077.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000095.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000021.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000052.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000011.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000067.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000031.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000082.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000046.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000014.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000073.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000028.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000051.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000035.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000013.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000048.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000072.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000060.JPG  \n",
            "  inflating: garbage_dataset/batch_4/000050.JPG  \n",
            "   creating: garbage_dataset/batch_12/\n",
            "  inflating: garbage_dataset/batch_12/000023.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000021.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000006.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000025.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000071.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000094.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000013.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000024.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000064.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000066.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000063.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000078.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000052.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000016.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000069.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000077.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000056.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000065.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000057.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000028.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000030.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000011.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000046.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000019.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000083.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000093.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000020.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000092.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000096.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000032.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000014.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000087.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000061.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000090.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000097.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000022.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000074.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000088.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000044.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000000.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000004.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000062.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000001.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000076.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000038.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000054.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000007.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000026.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000073.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000099.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000037.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000018.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000085.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000017.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000035.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000041.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000008.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000082.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000089.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000095.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000060.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000045.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000072.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000049.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000051.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000080.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000079.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000010.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000003.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000039.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000009.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000036.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000075.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000067.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000086.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000098.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000002.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000070.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000029.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000033.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000050.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000034.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000059.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000053.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000043.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000058.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000048.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000042.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000012.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000015.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000084.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000047.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000068.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000081.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000040.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000055.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000031.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000091.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000027.jpg  \n",
            "  inflating: garbage_dataset/batch_12/000005.jpg  \n",
            "   creating: garbage_dataset/batch_8/\n",
            "  inflating: garbage_dataset/batch_8/000023.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000021.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000006.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000025.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000071.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000094.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000013.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000024.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000064.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000066.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000063.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000078.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000052.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000016.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000069.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000077.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000056.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000065.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000057.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000028.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000030.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000011.jpg  \n",
            "  inflating: garbage_dataset/batch_8/a.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000046.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000019.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000083.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000093.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000020.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000092.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000096.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000032.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000014.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000087.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000061.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000090.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000097.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000022.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000074.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000088.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000044.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000000.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000004.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000062.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000001.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000076.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000038.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000054.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000007.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000026.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000073.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000099.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000037.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000018.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000085.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000017.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000035.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000041.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000008.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000082.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000089.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000095.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000060.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000045.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000072.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000049.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000051.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000080.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000079.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000010.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000003.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000039.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000009.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000036.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000075.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000067.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000086.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000098.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000002.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000070.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000029.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000033.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000050.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000034.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000059.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000053.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000043.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000058.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000048.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000042.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000012.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000015.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000084.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000047.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000068.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000081.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000040.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000055.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000031.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000091.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000027.jpg  \n",
            "  inflating: garbage_dataset/batch_8/000005.jpg  \n",
            "   creating: garbage_dataset/batch_6/\n",
            "  inflating: garbage_dataset/batch_6/000068.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000007.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000054.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000097.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000087.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000092.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000089.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000057.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000094.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000063.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000093.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000075.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000090.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000029.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000064.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000019.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000006.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000040.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000104.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000018.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000071.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000058.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000049.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000065.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000079.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000005.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000034.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000008.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000026.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000027.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000047.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000010.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000045.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000100.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000041.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000076.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000009.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000098.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000032.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000002.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000086.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000083.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000017.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000000.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000085.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000070.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000024.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000096.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000037.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000074.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000022.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000069.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000015.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000003.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000088.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000020.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000033.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000056.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000061.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000036.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000080.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000091.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000023.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000042.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000099.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000025.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000043.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000055.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000062.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000039.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000053.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000059.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000066.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000078.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000077.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000095.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000001.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000021.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000038.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000052.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000102.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000011.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000031.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000082.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000101.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000046.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000014.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000073.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000028.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000051.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000035.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000013.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000048.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000072.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000060.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000103.JPG  \n",
            "  inflating: garbage_dataset/batch_6/000050.JPG  \n",
            "   creating: garbage_dataset/batch_10/\n",
            "  inflating: garbage_dataset/batch_10/000023.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000021.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000006.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000025.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000071.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000094.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000013.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000024.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000064.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000066.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000063.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000078.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000052.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000016.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000069.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000077.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000056.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000065.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000057.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000028.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000030.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000011.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000046.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000019.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000083.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000093.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000020.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000092.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000096.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000032.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000014.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000087.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000061.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000090.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000097.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000022.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000074.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000088.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000044.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000000.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000004.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000062.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000001.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000076.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000038.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000054.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000007.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000026.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000073.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000099.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000037.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000018.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000085.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000017.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000035.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000041.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000008.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000082.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000089.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000095.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000060.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000045.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000072.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000049.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000051.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000080.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000079.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000010.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000003.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000039.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000009.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000036.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000075.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000067.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000086.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000098.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000002.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000070.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000029.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000033.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000050.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000034.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000059.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000053.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000043.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000058.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000048.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000042.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000012.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000015.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000084.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000047.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000068.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000081.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000040.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000055.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000031.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000091.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000027.jpg  \n",
            "  inflating: garbage_dataset/batch_10/000005.jpg  \n",
            "   creating: garbage_dataset/batch_13/\n",
            "  inflating: garbage_dataset/batch_13/000023.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000021.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000006.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000025.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000071.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000094.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000013.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000024.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000064.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000066.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000063.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000078.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000052.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000016.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000069.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000077.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000056.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000065.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000057.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000028.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000030.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000011.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000046.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000019.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000083.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000093.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000020.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000092.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000096.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000032.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000014.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000087.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000061.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000090.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000097.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000022.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000074.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000088.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000044.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000000.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000004.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000062.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000001.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000076.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000038.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000054.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000007.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000026.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000073.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000099.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000037.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000018.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000085.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000017.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000035.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000041.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000008.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000082.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000089.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000095.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000060.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000045.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000072.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000049.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000051.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000080.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000079.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000010.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000003.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000039.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000009.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000036.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000075.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000067.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000086.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000098.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000002.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000070.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000029.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000033.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000050.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000034.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000059.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000053.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000043.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000058.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000048.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000042.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000012.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000015.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000084.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000047.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000068.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000081.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000040.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000055.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000031.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000091.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000027.jpg  \n",
            "  inflating: garbage_dataset/batch_13/000005.jpg  \n",
            "   creating: garbage_dataset/batch_9/\n",
            "  inflating: garbage_dataset/batch_9/000023.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000021.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000006.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000025.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000071.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000094.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000013.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000024.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000064.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000066.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000063.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000078.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000052.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000016.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000069.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000077.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000056.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000065.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000057.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000028.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000030.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000011.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000046.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000019.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000083.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000093.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000020.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000092.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000096.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000032.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000014.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000087.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000061.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000090.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000097.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000022.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000074.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000088.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000044.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000000.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000004.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000062.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000001.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000076.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000038.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000054.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000007.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000026.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000073.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000099.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000037.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000018.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000085.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000017.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000035.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000041.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000008.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000082.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000089.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000095.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000060.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000045.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000072.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000049.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000051.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000080.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000079.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000010.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000003.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000039.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000009.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000036.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000075.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000067.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000086.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000098.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000002.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000070.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000029.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000033.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000050.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000034.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000059.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000053.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000043.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000058.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000048.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000042.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000012.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000015.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000084.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000047.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000068.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000081.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000040.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000055.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000031.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000091.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000027.jpg  \n",
            "  inflating: garbage_dataset/batch_9/000005.jpg  \n",
            "   creating: garbage_dataset/batch_15/\n",
            "  inflating: garbage_dataset/batch_15/000023.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000021.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000006.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000025.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000071.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000013.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000024.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000064.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000066.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000063.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000078.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000052.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000016.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000069.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000077.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000056.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000065.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000057.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000028.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000030.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000011.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000046.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000019.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000083.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000020.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000032.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000014.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000061.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000022.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000074.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000044.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000000.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000004.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000062.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000001.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000076.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000038.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000054.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000007.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000026.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000073.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000037.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000018.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000017.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000035.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000041.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000008.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000082.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000060.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000045.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000072.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000049.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000051.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000080.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000079.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000010.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000003.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000039.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000009.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000036.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000075.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000067.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000002.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000070.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000029.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000033.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000050.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000034.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000059.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000053.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000043.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000058.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000048.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000042.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000012.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000015.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000084.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000047.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000068.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000081.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000040.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000055.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000031.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000027.jpg  \n",
            "  inflating: garbage_dataset/batch_15/000005.jpg  \n",
            "   creating: garbage_dataset/batch_2/\n",
            "  inflating: garbage_dataset/batch_2/000068.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000007.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000054.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000097.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000092.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000089.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000016.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000057.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000094.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000063.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000093.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000075.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000090.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000029.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000064.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000019.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000006.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000040.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000018.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000012.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000071.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000058.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000049.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000065.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000079.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000005.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000044.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000034.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000008.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000026.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000027.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000047.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000010.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000041.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000076.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000009.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000098.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000032.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000086.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000083.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000017.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000000.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000085.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000070.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000024.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000096.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000037.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000074.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000022.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000069.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000015.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000081.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000003.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000084.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000088.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000020.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000033.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000056.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000061.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000036.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000080.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000091.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000023.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000042.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000099.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000025.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000043.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000055.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000062.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000039.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000053.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000059.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000030.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000077.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000095.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000001.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000021.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000038.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000052.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000067.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000031.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000082.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000046.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000014.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000073.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000051.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000035.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000013.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000048.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000072.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000060.JPG  \n",
            "  inflating: garbage_dataset/batch_2/000050.JPG  \n",
            "   creating: garbage_dataset/batch_11/\n",
            "  inflating: garbage_dataset/batch_11/000023.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000021.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000006.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000025.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000071.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000094.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000013.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000024.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000064.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000066.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000063.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000078.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000052.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000016.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000069.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000077.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000056.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000065.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000057.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000028.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000030.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000011.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000046.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000019.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000083.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000093.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000020.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000092.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000096.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000032.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000014.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000087.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000061.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000090.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000097.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000022.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000074.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000088.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000044.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000000.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000004.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000062.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000001.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000076.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000038.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000054.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000007.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000026.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000073.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000099.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000037.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000018.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000085.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000017.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000035.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000041.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000008.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000082.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000089.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000095.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000060.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000045.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000072.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000049.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000051.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000080.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000079.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000010.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000003.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000039.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000009.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000036.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000075.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000067.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000086.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000098.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000002.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000070.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000029.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000033.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000050.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000034.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000059.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000053.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000043.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000058.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000048.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000042.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000012.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000015.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000084.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000047.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000068.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000081.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000040.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000055.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000031.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000091.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000027.jpg  \n",
            "  inflating: garbage_dataset/batch_11/000005.jpg  \n",
            "   creating: garbage_dataset/batch_3/\n",
            "  inflating: garbage_dataset/batch_3/IMG_5063.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5039.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5048.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4891.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5065.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4916.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5052.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4855.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4965.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5002.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4854.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5049.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4967.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4928.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4980.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4966.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4907.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4994.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5043.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5042.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5045.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4936.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5037.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4978.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4862.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5056.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4998.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4932.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5040.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4865.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4887.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4972.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5046.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4996.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4924.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5003.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4939.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5064.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4971.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4941.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4950.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4917.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5068.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4879.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5044.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4889.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4911.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4901.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5061.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4883.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4859.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4877.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5054.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4934.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5057.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4875.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4860.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4964.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4948.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4997.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4868.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5067.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4919.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4876.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4969.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5050.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5036.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5055.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5041.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4914.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4915.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5058.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5060.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4898.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5051.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4869.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4874.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4893.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4922.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4902.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5066.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4921.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4881.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4913.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4856.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4878.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4961.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4929.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4963.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4895.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4992.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_5053.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4857.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4897.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4977.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4926.JPG  \n",
            "  inflating: garbage_dataset/batch_3/IMG_4852.JPG  \n",
            "   creating: garbage_dataset/batch_1/\n",
            "  inflating: garbage_dataset/batch_1/000107.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000068.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000023.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000127.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000021.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000006.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000025.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000124.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000122.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000087.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000128.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000092.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000013.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000024.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000016.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000094.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000056.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000028.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000030.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000117.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000093.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000090.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000011.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000064.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000019.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000110.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000032.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000014.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000104.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000061.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000022.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000071.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000000.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000004.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000065.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000001.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000079.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000038.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000054.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000007.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000026.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000108.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000100.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000076.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000098.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000037.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000086.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000120.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000083.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000017.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000085.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000035.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000070.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000096.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000008.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000074.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000069.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000060.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000081.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000129.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000045.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000084.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000088.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000049.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000010.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000003.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000111.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000119.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000091.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000099.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000062.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000066.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000078.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000029.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000095.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000105.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000050.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000059.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000053.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000043.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000102.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000067.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000058.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000048.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000121.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000082.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000101.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000042.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000115.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000012.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000015.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000047.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000073.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000072.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000118.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000040.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000055.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000031.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000106.JPG  \n",
            "  inflating: garbage_dataset/batch_1/000027.jpg  \n",
            "  inflating: garbage_dataset/batch_1/000005.jpg  \n",
            "   creating: garbage_dataset/batch_5/\n",
            "  inflating: garbage_dataset/batch_5/000107.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000068.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000007.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000054.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000097.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000113.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000087.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000092.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000089.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000016.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000057.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000094.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000063.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000117.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000093.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000075.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000090.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000029.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000004.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000064.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000114.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000110.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000019.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000006.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000040.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000104.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000018.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000012.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000071.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000058.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000049.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000079.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000005.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000034.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000008.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000026.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000027.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000108.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000047.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000010.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000045.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000100.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000041.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000076.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000009.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000098.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000002.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000086.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000120.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000083.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000017.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000000.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000085.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000070.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000024.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000096.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000037.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000116.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000074.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000022.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000069.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000015.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000081.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000084.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000088.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000020.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000033.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000056.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000111.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000119.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000061.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000036.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000091.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000023.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000042.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000099.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000025.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000043.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000055.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000062.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000039.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000059.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000066.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000030.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000095.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000001.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000105.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000021.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000038.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000052.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000102.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000011.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000067.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000031.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000112.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000082.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000101.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000046.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000115.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000014.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000073.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000028.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000051.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000035.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000013.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000048.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000072.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000118.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000106.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000060.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000103.JPG  \n",
            "  inflating: garbage_dataset/batch_5/000050.JPG  \n",
            "  inflating: garbage_dataset/annotations.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhkLHaswFcms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm /content/garbage_detection/data/data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUPUH-WIKq0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !mv /contnt/TACO/data /content/garbage_detection/data/garbage_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6ufDU8p8HZKn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "78040b64-b7b1-4180-af27-65fb399006f8"
      },
      "source": [
        "%cd /content/garbage_detection\n",
        "!python generate_tfrecord.py --csv_input=train_labels.csv --output_path=train.record --img_path=data/garbage_dataset --label_map=label_map.pbtxt\n",
        "!python generate_tfrecord.py --csv_input=test_labels.csv --output_path=test.record --img_path=data/garbage_dataset --label_map=label_map.pbtxt "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/garbage_detection\n",
            "WARNING:tensorflow:From generate_tfrecord.py:134: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From generate_tfrecord.py:107: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W0604 20:13:15.042225 139950251493248 module_wrapper.py:139] From generate_tfrecord.py:107: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From generate_tfrecord.py:53: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0604 20:13:15.807020 139950251493248 module_wrapper.py:139] From generate_tfrecord.py:53: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/garbage_detection/train.record\n",
            "WARNING:tensorflow:From generate_tfrecord.py:134: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From generate_tfrecord.py:107: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W0604 20:14:12.898944 140496295675776 module_wrapper.py:139] From generate_tfrecord.py:107: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From generate_tfrecord.py:53: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0604 20:14:13.017151 140496295675776 module_wrapper.py:139] From generate_tfrecord.py:53: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/garbage_detection/test.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "32kIm0JHOeBe",
        "colab": {}
      },
      "source": [
        "test_record_fname = '/content/garbage_detection/test.record'\n",
        "train_record_fname = '/content/garbage_detection/train.record'\n",
        "label_map_pbtxt_fname = '/content/garbage_detection/label_map.pbtxt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sKiB2GzkQqUy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "43567d6c-7eb1-4439-fe38-0c9799fb7d53"
      },
      "source": [
        "!wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-04 20:14:17--  http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 108.177.112.128, 2607:f8b0:4001:c12::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|108.177.112.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 187925923 (179M) [application/x-tar]\n",
            "Saving to: ‘ssd_mobilenet_v2_coco_2018_03_29.tar.gz’\n",
            "\n",
            "ssd_mobilenet_v2_co 100%[===================>] 179.22M   110MB/s    in 1.6s    \n",
            "\n",
            "2020-06-04 20:14:19 (110 MB/s) - ‘ssd_mobilenet_v2_coco_2018_03_29.tar.gz’ saved [187925923/187925923]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ALzYe_MFRkBz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "96e2b960-a060-425a-90f3-2b90339e28e8"
      },
      "source": [
        "!tar -xzvf ssd_mobilenet_v2_coco_2018_03_29.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ssd_mobilenet_v2_coco_2018_03_29/checkpoint\n",
            "ssd_mobilenet_v2_coco_2018_03_29/model.ckpt.meta\n",
            "ssd_mobilenet_v2_coco_2018_03_29/pipeline.config\n",
            "ssd_mobilenet_v2_coco_2018_03_29/saved_model/saved_model.pb\n",
            "ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb\n",
            "ssd_mobilenet_v2_coco_2018_03_29/saved_model/\n",
            "ssd_mobilenet_v2_coco_2018_03_29/saved_model/variables/\n",
            "ssd_mobilenet_v2_coco_2018_03_29/model.ckpt.index\n",
            "ssd_mobilenet_v2_coco_2018_03_29/\n",
            "ssd_mobilenet_v2_coco_2018_03_29/model.ckpt.data-00000-of-00001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f_xVOeQxRr_t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79c570cf-7e1b-47dc-c631-24e0ef7b0691"
      },
      "source": [
        "fine_tune_checkpoint = \"/content/garbage_detection/ssd_mobilenet_v2_coco_2018_03_29/model.ckpt\"\n",
        "fine_tune_checkpoint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/garbage_detection/ssd_mobilenet_v2_coco_2018_03_29/model.ckpt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qOADBJhjSNwl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "7143174e-50c5-4796-a296-108760a698da"
      },
      "source": [
        "%cd /content\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -o ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "--2020-06-04 20:14:35--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.70.230.150, 34.233.91.203, 34.196.154.11, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.70.230.150|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  37.4MB/s    in 0.4s    \n",
            "\n",
            "2020-06-04 20:14:36 (37.4 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JoAfFkLVu9I6",
        "colab": {}
      },
      "source": [
        "# %cd /content/\n",
        "# !cp -r drive/My\\ Drive/training_logs .\n",
        "# !cp -r drive/My\\ Drive/trained_model ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OkZcg3coTdHK",
        "colab": {}
      },
      "source": [
        "# !mkdir training_logs\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir /content/drive/My\\ Drive/training_logs_cls --host 0.0.0.0 --port 6006 &'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BudAEzRSVI9q",
        "colab": {}
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DgjZUbOmVMvi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a0f91b9a-742e-46b6-bfd4-71600246f317"
      },
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://dfa36f230ca1.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BceXv9fYW1tg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc31bfce-540c-463e-ba92-dd46234cd47f"
      },
      "source": [
        "%cd /content\n",
        "!python /content/models/research/object_detection/model_main.py \\\n",
        "    --pipeline_config_path=garbage_detection/pipeline.config \\\n",
        "    --model_dir=drive/My\\ Drive/training_logs_cls \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps=40000 \\\n",
        "    --num_eval_steps=800"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/model_main.py:109: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0312 14:35:49.015900 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/model_lib.py:628: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
            "\n",
            "W0312 14:35:49.019287 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/model_lib.py:628: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
            "\n",
            "WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\n",
            "W0312 14:35:49.019417 139913100998528 model_lib.py:629] Forced number of epochs for all eval validations to be 1.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/config_util.py:488: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "W0312 14:35:49.019505 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/utils/config_util.py:488: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:Maybe overwriting train_steps: 40000\n",
            "I0312 14:35:49.019568 139913100998528 config_util.py:488] Maybe overwriting train_steps: 40000\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I0312 14:35:49.019650 139913100998528 config_util.py:488] Maybe overwriting use_bfloat16: False\n",
            "INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: 1\n",
            "I0312 14:35:49.019701 139913100998528 config_util.py:488] Maybe overwriting sample_1_of_n_eval_examples: 1\n",
            "INFO:tensorflow:Maybe overwriting eval_num_epochs: 1\n",
            "I0312 14:35:49.019752 139913100998528 config_util.py:488] Maybe overwriting eval_num_epochs: 1\n",
            "INFO:tensorflow:Maybe overwriting load_pretrained: True\n",
            "I0312 14:35:49.019798 139913100998528 config_util.py:488] Maybe overwriting load_pretrained: True\n",
            "INFO:tensorflow:Ignoring config override key: load_pretrained\n",
            "I0312 14:35:49.019844 139913100998528 config_util.py:498] Ignoring config override key: load_pretrained\n",
            "WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
            "W0312 14:35:49.020369 139913100998528 model_lib.py:645] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
            "INFO:tensorflow:create_estimator_and_inputs: use_tpu False, export_to_tpu False\n",
            "I0312 14:35:49.020439 139913100998528 model_lib.py:680] create_estimator_and_inputs: use_tpu False, export_to_tpu False\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'drive/My Drive/training_logs_cls', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3fa15e2198>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "I0312 14:35:49.020816 139913100998528 estimator.py:212] Using config: {'_model_dir': 'drive/My Drive/training_logs_cls', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3fa15e2198>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f3fa1bcd840>) includes params argument, but params are not passed to Estimator.\n",
            "W0312 14:35:49.021055 139913100998528 model_fn.py:630] Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f3fa1bcd840>) includes params argument, but params are not passed to Estimator.\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "I0312 14:35:49.021673 139913100998528 estimator_training.py:186] Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "I0312 14:35:49.021813 139913100998528 training.py:612] Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "I0312 14:35:49.022017 139913100998528 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W0312 14:35:50.786944 139913100998528 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/data_decoders/tf_example_decoder.py:182: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0312 14:35:50.801223 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/data_decoders/tf_example_decoder.py:182: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/data_decoders/tf_example_decoder.py:197: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
            "\n",
            "W0312 14:35:50.801458 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/data_decoders/tf_example_decoder.py:197: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0312 14:35:50.813643 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W0312 14:35:50.814837 139913100998528 dataset_builder.py:72] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W0312 14:35:50.819693 139913100998528 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0312 14:35:50.819854 139913100998528 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0312 14:35:50.839026 139913100998528 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.logging.warn is deprecated. Please use tf.compat.v1.logging.warn instead.\n",
            "\n",
            "W0312 14:35:52.250827 139913100998528 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.logging.warn is deprecated. Please use tf.compat.v1.logging.warn instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.\n",
            "\n",
            "W0312 14:35:59.026168 139913100998528 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0312 14:35:59.105533 139913100998528 deprecation.py:323] From /content/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0312 14:36:01.096671 139913100998528 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/operators/control_flow.py:1004: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W0312 14:36:04.220046 139913100998528 api.py:332] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/operators/control_flow.py:1004: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W0312 14:36:07.180518 139913100998528 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "W0312 14:36:07.181440 139913100998528 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:166: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0312 14:36:07.545547 139913100998528 deprecation.py:323] From /content/models/research/object_detection/inputs.py:166: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.string_to_hash_bucket_fast is deprecated. Please use tf.strings.to_hash_bucket_fast instead.\n",
            "\n",
            "W0312 14:36:09.150727 139913100998528 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.string_to_hash_bucket_fast is deprecated. Please use tf.strings.to_hash_bucket_fast instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:158: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.batch(..., drop_remainder=True)`.\n",
            "W0312 14:36:09.611135 139913100998528 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:158: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.batch(..., drop_remainder=True)`.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0312 14:36:09.623306 139913100998528 estimator.py:1148] Calling model_fn.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/meta_architectures/ssd_meta_arch.py:589: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0312 14:36:10.405176 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/meta_architectures/ssd_meta_arch.py:589: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/meta_architectures/ssd_meta_arch.py:597: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0312 14:36:10.405428 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/meta_architectures/ssd_meta_arch.py:597: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W0312 14:36:10.408146 139913100998528 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "W0312 14:36:12.962256 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 14:36:12.971679 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 14:36:12.999132 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 14:36:13.025774 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 14:36:13.052699 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 14:36:13.079639 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 14:36:13.112112 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/variables_helper.py:179: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "W0312 14:36:13.146121 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/utils/variables_helper.py:179: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/variables_helper.py:139: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
            "\n",
            "W0312 14:36:13.147197 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/utils/variables_helper.py:139: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/model_lib.py:353: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
            "\n",
            "W0312 14:36:13.151997 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/model_lib.py:353: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/box_coders/faster_rcnn_box_coder.py:82: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0312 14:36:14.080629 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/box_coders/faster_rcnn_box_coder.py:82: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/meta_architectures/ssd_meta_arch.py:1163: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0312 14:36:23.131414 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/meta_architectures/ssd_meta_arch.py:1163: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/core/losses.py:177: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
            "\n",
            "W0312 14:36:23.136388 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/core/losses.py:177: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/core/losses.py:183: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
            "\n",
            "W0312 14:36:23.137430 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/core/losses.py:183: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/meta_architectures/ssd_meta_arch.py:1275: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "W0312 14:36:24.483336 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/meta_architectures/ssd_meta_arch.py:1275: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/model_lib.py:380: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0312 14:36:24.485675 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/model_lib.py:380: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/learning_schedules.py:66: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "W0312 14:36:24.485926 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/utils/learning_schedules.py:66: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/optimizer_builder.py:47: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n",
            "W0312 14:36:24.493059 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/builders/optimizer_builder.py:47: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/model_lib.py:398: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "W0312 14:36:24.493280 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/model_lib.py:398: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0312 14:36:27.875056 139913100998528 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/model_lib.py:515: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W0312 14:36:33.242094 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/model_lib.py:515: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/model_lib.py:519: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W0312 14:36:33.915091 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/model_lib.py:519: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/model_lib.py:520: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.\n",
            "\n",
            "W0312 14:36:33.915321 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/model_lib.py:520: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.\n",
            "\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0312 14:36:33.915863 139913100998528 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "I0312 14:36:33.917101 139913100998528 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0312 14:36:39.018498 139913100998528 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-12 14:36:39.039052: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2020-03-12 14:36:39.041008: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x230e3dc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-03-12 14:36:39.041046: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-03-12 14:36:39.064830: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-03-12 14:36:39.229221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 14:36:39.229995: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x230e3c00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-03-12 14:36:39.230023: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-03-12 14:36:39.234153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 14:36:39.234818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-12 14:36:39.256801: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-12 14:36:39.495868: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-12 14:36:39.634519: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-12 14:36:39.656935: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-12 14:36:39.934800: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-12 14:36:39.956751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-12 14:36:40.472891: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-12 14:36:40.473105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 14:36:40.473785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 14:36:40.474276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-12 14:36:40.477925: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-12 14:36:40.479423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-12 14:36:40.479457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-12 14:36:40.479472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-12 14:36:40.485752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 14:36:40.487286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 14:36:40.491185: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-03-12 14:36:40.491240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-20267\n",
            "I0312 14:36:40.498250 139913100998528 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-20267\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "W0312 14:36:50.034903 139913100998528 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0312 14:36:51.400217 139913100998528 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0312 14:36:51.962150 139913100998528 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 20267 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "I0312 14:37:06.464230 139913100998528 basic_session_run_hooks.py:606] Saving checkpoints for 20267 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "2020-03-12 14:37:26.977857: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 23970816 exceeds 10% of system memory.\n",
            "2020-03-12 14:37:26.978204: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 38937600 exceeds 10% of system memory.\n",
            "2020-03-12 14:37:26.982035: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 23970816 exceeds 10% of system memory.\n",
            "2020-03-12 14:37:26.982048: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 38937600 exceeds 10% of system memory.\n",
            "2020-03-12 14:37:27.046883: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 95883264 exceeds 10% of system memory.\n",
            "2020-03-12 14:38:02.661829: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-12 14:38:07.374571: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "INFO:tensorflow:loss = 2.6097038, step = 20267\n",
            "I0312 14:38:10.601855 139913100998528 basic_session_run_hooks.py:262] loss = 2.6097038, step = 20267\n",
            "INFO:tensorflow:global_step/sec: 0.29015\n",
            "I0312 14:43:55.250352 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.29015\n",
            "INFO:tensorflow:loss = 2.7408237, step = 20367 (344.650 sec)\n",
            "I0312 14:43:55.252188 139913100998528 basic_session_run_hooks.py:260] loss = 2.7408237, step = 20367 (344.650 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 20427 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "I0312 14:47:10.934483 139913100998528 basic_session_run_hooks.py:606] Saving checkpoints for 20427 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "W0312 14:47:11.446262 139913100998528 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0312 14:47:14.339241 139913100998528 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 14:47:16.393020 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 14:47:16.424317 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 14:47:16.454837 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 14:47:16.483681 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 14:47:16.510921 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 14:47:16.538476 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/eval_util.py:796: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0312 14:47:17.393595 139913100998528 deprecation.py:323] From /content/models/research/object_detection/eval_util.py:796: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/visualization_utils.py:498: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "W0312 14:47:17.608001 139913100998528 deprecation.py:323] From /content/models/research/object_detection/utils/visualization_utils.py:498: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/visualization_utils.py:1044: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
            "\n",
            "W0312 14:47:17.750277 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/utils/visualization_utils.py:1044: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/model_lib.py:484: The name tf.metrics.mean is deprecated. Please use tf.compat.v1.metrics.mean instead.\n",
            "\n",
            "W0312 14:47:17.831971 139913100998528 module_wrapper.py:139] From /content/models/research/object_detection/model_lib.py:484: The name tf.metrics.mean is deprecated. Please use tf.compat.v1.metrics.mean instead.\n",
            "\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0312 14:47:18.141089 139913100998528 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-03-12T14:47:18Z\n",
            "I0312 14:47:18.156079 139913100998528 evaluation.py:255] Starting evaluation at 2020-03-12T14:47:18Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0312 14:47:18.597774 139913100998528 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-12 14:47:18.598804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 14:47:18.599206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-12 14:47:18.599296: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-12 14:47:18.599315: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-12 14:47:18.599332: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-12 14:47:18.599349: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-12 14:47:18.599364: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-12 14:47:18.599380: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-12 14:47:18.599399: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-12 14:47:18.599458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 14:47:18.599838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 14:47:18.600163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-12 14:47:18.600242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-12 14:47:18.600254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-12 14:47:18.600261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-12 14:47:18.600334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 14:47:18.600715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 14:47:18.601052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-20427\n",
            "I0312 14:47:18.603239 139913100998528 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-20427\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0312 14:47:19.547511 139913100998528 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0312 14:47:19.676643 139913100998528 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 208 images.\n",
            "I0312 14:48:56.078587 139909166556928 coco_evaluation.py:205] Performing evaluation on 208 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0312 14:48:56.079484 139909166556928 coco_tools.py:115] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.01s)\n",
            "I0312 14:48:56.092104 139909166556928 coco_tools.py:137] DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.33s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.010\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.014\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.013\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.012\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.034\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.040\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.041\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.006\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.050\n",
            "INFO:tensorflow:Finished evaluation at 2020-03-12-14:48:58\n",
            "I0312 14:48:58.552992 139913100998528 evaluation.py:275] Finished evaluation at 2020-03-12-14:48:58\n",
            "INFO:tensorflow:Saving dict for global step 20427: DetectionBoxes_Precision/mAP = 0.009594971, DetectionBoxes_Precision/mAP (large) = 0.012158674, DetectionBoxes_Precision/mAP (medium) = 0.00022330048, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.013968985, DetectionBoxes_Precision/mAP@.75IOU = 0.012549749, DetectionBoxes_Recall/AR@1 = 0.03389257, DetectionBoxes_Recall/AR@10 = 0.039948247, DetectionBoxes_Recall/AR@100 = 0.04094023, DetectionBoxes_Recall/AR@100 (large) = 0.050416667, DetectionBoxes_Recall/AR@100 (medium) = 0.0060457517, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 21.692081, Loss/localization_loss = 3.3332715, Loss/regularization_loss = 0.26729578, Loss/total_loss = 25.292646, global_step = 20427, learning_rate = 0.004, loss = 25.292646\n",
            "I0312 14:48:58.553253 139913100998528 estimator.py:2049] Saving dict for global step 20427: DetectionBoxes_Precision/mAP = 0.009594971, DetectionBoxes_Precision/mAP (large) = 0.012158674, DetectionBoxes_Precision/mAP (medium) = 0.00022330048, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.013968985, DetectionBoxes_Precision/mAP@.75IOU = 0.012549749, DetectionBoxes_Recall/AR@1 = 0.03389257, DetectionBoxes_Recall/AR@10 = 0.039948247, DetectionBoxes_Recall/AR@100 = 0.04094023, DetectionBoxes_Recall/AR@100 (large) = 0.050416667, DetectionBoxes_Recall/AR@100 (medium) = 0.0060457517, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 21.692081, Loss/localization_loss = 3.3332715, Loss/regularization_loss = 0.26729578, Loss/total_loss = 25.292646, global_step = 20427, learning_rate = 0.004, loss = 25.292646\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 20427: drive/My Drive/training_logs_cls/model.ckpt-20427\n",
            "I0312 14:48:59.663823 139913100998528 estimator.py:2109] Saving 'checkpoint_path' summary for global step 20427: drive/My Drive/training_logs_cls/model.ckpt-20427\n",
            "INFO:tensorflow:global_step/sec: 0.227512\n",
            "I0312 14:51:14.787392 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.227512\n",
            "INFO:tensorflow:loss = 3.0899646, step = 20467 (439.537 sec)\n",
            "I0312 14:51:14.789039 139913100998528 basic_session_run_hooks.py:260] loss = 3.0899646, step = 20467 (439.537 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.298999\n",
            "I0312 14:56:49.236254 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.298999\n",
            "INFO:tensorflow:loss = 2.6982567, step = 20567 (334.449 sec)\n",
            "I0312 14:56:49.237830 139913100998528 basic_session_run_hooks.py:260] loss = 2.6982567, step = 20567 (334.449 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 20575 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "I0312 14:57:13.402368 139913100998528 basic_session_run_hooks.py:606] Saving checkpoints for 20575 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0312 14:57:16.422250 139913100998528 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 14:57:18.586583 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 14:57:18.615051 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 14:57:18.642930 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 14:57:18.672635 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 14:57:18.703386 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 14:57:18.730806 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0312 14:57:20.260829 139913100998528 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-03-12T14:57:20Z\n",
            "I0312 14:57:20.276927 139913100998528 evaluation.py:255] Starting evaluation at 2020-03-12T14:57:20Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0312 14:57:20.690943 139913100998528 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-12 14:57:20.691560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 14:57:20.692001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-12 14:57:20.692083: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-12 14:57:20.692106: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-12 14:57:20.692129: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-12 14:57:20.692149: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-12 14:57:20.692165: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-12 14:57:20.692179: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-12 14:57:20.692194: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-12 14:57:20.692262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 14:57:20.692639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 14:57:20.692962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-12 14:57:20.693009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-12 14:57:20.693021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-12 14:57:20.693027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-12 14:57:20.693096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 14:57:20.693461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 14:57:20.693806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-20575\n",
            "I0312 14:57:20.695691 139913100998528 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-20575\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0312 14:57:21.590492 139913100998528 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0312 14:57:21.715403 139913100998528 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 208 images.\n",
            "I0312 14:58:56.634995 139909183342336 coco_evaluation.py:205] Performing evaluation on 208 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0312 14:58:56.636003 139909183342336 coco_tools.py:115] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.02s)\n",
            "I0312 14:58:56.654324 139909183342336 coco_tools.py:137] DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.32s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.011\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.016\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.013\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.033\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.043\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.043\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.002\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.055\n",
            "INFO:tensorflow:Finished evaluation at 2020-03-12-14:58:59\n",
            "I0312 14:58:59.319589 139913100998528 evaluation.py:275] Finished evaluation at 2020-03-12-14:58:59\n",
            "INFO:tensorflow:Saving dict for global step 20575: DetectionBoxes_Precision/mAP = 0.011182128, DetectionBoxes_Precision/mAP (large) = 0.013510992, DetectionBoxes_Precision/mAP (medium) = 4.0412204e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.01585978, DetectionBoxes_Precision/mAP@.75IOU = 0.012937962, DetectionBoxes_Recall/AR@1 = 0.033342004, DetectionBoxes_Recall/AR@10 = 0.04275906, DetectionBoxes_Recall/AR@100 = 0.04314516, DetectionBoxes_Recall/AR@100 (large) = 0.05486111, DetectionBoxes_Recall/AR@100 (medium) = 0.0018518518, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 21.465212, Loss/localization_loss = 3.2199838, Loss/regularization_loss = 0.267395, Loss/total_loss = 24.952587, global_step = 20575, learning_rate = 0.004, loss = 24.952587\n",
            "I0312 14:58:59.319917 139913100998528 estimator.py:2049] Saving dict for global step 20575: DetectionBoxes_Precision/mAP = 0.011182128, DetectionBoxes_Precision/mAP (large) = 0.013510992, DetectionBoxes_Precision/mAP (medium) = 4.0412204e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.01585978, DetectionBoxes_Precision/mAP@.75IOU = 0.012937962, DetectionBoxes_Recall/AR@1 = 0.033342004, DetectionBoxes_Recall/AR@10 = 0.04275906, DetectionBoxes_Recall/AR@100 = 0.04314516, DetectionBoxes_Recall/AR@100 (large) = 0.05486111, DetectionBoxes_Recall/AR@100 (medium) = 0.0018518518, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 21.465212, Loss/localization_loss = 3.2199838, Loss/regularization_loss = 0.267395, Loss/total_loss = 24.952587, global_step = 20575, learning_rate = 0.004, loss = 24.952587\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 20575: drive/My Drive/training_logs_cls/model.ckpt-20575\n",
            "I0312 14:58:59.330560 139913100998528 estimator.py:2109] Saving 'checkpoint_path' summary for global step 20575: drive/My Drive/training_logs_cls/model.ckpt-20575\n",
            "INFO:tensorflow:global_step/sec: 0.231887\n",
            "I0312 15:04:00.481088 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.231887\n",
            "INFO:tensorflow:loss = 3.0554752, step = 20667 (431.245 sec)\n",
            "I0312 15:04:00.482540 139913100998528 basic_session_run_hooks.py:260] loss = 3.0554752, step = 20667 (431.245 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 20729 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "I0312 15:07:14.588215 139913100998528 basic_session_run_hooks.py:606] Saving checkpoints for 20729 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0312 15:07:17.693372 139913100998528 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:07:19.781256 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:07:19.809483 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:07:19.836780 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:07:19.866919 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:07:19.894086 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:07:19.921350 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0312 15:07:21.365572 139913100998528 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-03-12T15:07:21Z\n",
            "I0312 15:07:21.380527 139913100998528 evaluation.py:255] Starting evaluation at 2020-03-12T15:07:21Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0312 15:07:21.792804 139913100998528 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-12 15:07:21.793466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:07:21.793915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-12 15:07:21.794014: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-12 15:07:21.794040: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-12 15:07:21.794065: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-12 15:07:21.794088: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-12 15:07:21.794111: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-12 15:07:21.794131: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-12 15:07:21.794152: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-12 15:07:21.794250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:07:21.794671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:07:21.795027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-12 15:07:21.795159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-12 15:07:21.795175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-12 15:07:21.795184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-12 15:07:21.795293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:07:21.795702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:07:21.796061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-20729\n",
            "I0312 15:07:21.797906 139913100998528 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-20729\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0312 15:07:22.739934 139913100998528 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0312 15:07:22.873739 139913100998528 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 208 images.\n",
            "I0312 15:08:54.556976 139909191735040 coco_evaluation.py:205] Performing evaluation on 208 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0312 15:08:54.557883 139909191735040 coco_tools.py:115] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.01s)\n",
            "I0312 15:08:54.572155 139909191735040 coco_tools.py:137] DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.22s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.30s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.012\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.018\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.013\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.016\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.041\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.052\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.053\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.066\n",
            "INFO:tensorflow:Finished evaluation at 2020-03-12-15:08:57\n",
            "I0312 15:08:57.004701 139913100998528 evaluation.py:275] Finished evaluation at 2020-03-12-15:08:57\n",
            "INFO:tensorflow:Saving dict for global step 20729: DetectionBoxes_Precision/mAP = 0.011712876, DetectionBoxes_Precision/mAP (large) = 0.015594132, DetectionBoxes_Precision/mAP (medium) = 0.00024822995, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.01840267, DetectionBoxes_Precision/mAP@.75IOU = 0.01326462, DetectionBoxes_Recall/AR@1 = 0.04115315, DetectionBoxes_Recall/AR@10 = 0.052413195, DetectionBoxes_Recall/AR@100 = 0.052670594, DetectionBoxes_Recall/AR@100 (large) = 0.065694444, DetectionBoxes_Recall/AR@100 (medium) = 0.004938272, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 21.126472, Loss/localization_loss = 3.440734, Loss/regularization_loss = 0.26749796, Loss/total_loss = 24.834715, global_step = 20729, learning_rate = 0.004, loss = 24.834715\n",
            "I0312 15:08:57.004954 139913100998528 estimator.py:2049] Saving dict for global step 20729: DetectionBoxes_Precision/mAP = 0.011712876, DetectionBoxes_Precision/mAP (large) = 0.015594132, DetectionBoxes_Precision/mAP (medium) = 0.00024822995, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.01840267, DetectionBoxes_Precision/mAP@.75IOU = 0.01326462, DetectionBoxes_Recall/AR@1 = 0.04115315, DetectionBoxes_Recall/AR@10 = 0.052413195, DetectionBoxes_Recall/AR@100 = 0.052670594, DetectionBoxes_Recall/AR@100 (large) = 0.065694444, DetectionBoxes_Recall/AR@100 (medium) = 0.004938272, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 21.126472, Loss/localization_loss = 3.440734, Loss/regularization_loss = 0.26749796, Loss/total_loss = 24.834715, global_step = 20729, learning_rate = 0.004, loss = 24.834715\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 20729: drive/My Drive/training_logs_cls/model.ckpt-20729\n",
            "I0312 15:08:57.014752 139913100998528 estimator.py:2109] Saving 'checkpoint_path' summary for global step 20729: drive/My Drive/training_logs_cls/model.ckpt-20729\n",
            "INFO:tensorflow:global_step/sec: 0.235929\n",
            "I0312 15:11:04.338006 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.235929\n",
            "INFO:tensorflow:loss = 3.2364814, step = 20767 (423.857 sec)\n",
            "I0312 15:11:04.339361 139913100998528 basic_session_run_hooks.py:260] loss = 3.2364814, step = 20767 (423.857 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.30509\n",
            "I0312 15:16:32.110540 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.30509\n",
            "INFO:tensorflow:loss = 3.0077033, step = 20867 (327.773 sec)\n",
            "I0312 15:16:32.112403 139913100998528 basic_session_run_hooks.py:260] loss = 3.0077033, step = 20867 (327.773 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 20881 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "I0312 15:17:14.639530 139913100998528 basic_session_run_hooks.py:606] Saving checkpoints for 20881 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "I0312 15:17:17.094531 139913100998528 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "INFO:tensorflow:global_step/sec: 0.303375\n",
            "I0312 15:22:01.735075 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.303375\n",
            "INFO:tensorflow:loss = 2.6807232, step = 20967 (329.624 sec)\n",
            "I0312 15:22:01.736283 139913100998528 basic_session_run_hooks.py:260] loss = 2.6807232, step = 20967 (329.624 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 21065 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "I0312 15:27:15.662006 139913100998528 basic_session_run_hooks.py:606] Saving checkpoints for 21065 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0312 15:27:18.751977 139913100998528 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:27:20.782590 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:27:20.812929 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:27:20.841996 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:27:20.868810 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:27:20.895564 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:27:20.922699 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0312 15:27:22.398037 139913100998528 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-03-12T15:27:22Z\n",
            "I0312 15:27:22.412959 139913100998528 evaluation.py:255] Starting evaluation at 2020-03-12T15:27:22Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0312 15:27:22.809011 139913100998528 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-12 15:27:22.809768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:27:22.810210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-12 15:27:22.810328: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-12 15:27:22.810347: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-12 15:27:22.810371: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-12 15:27:22.810400: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-12 15:27:22.810413: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-12 15:27:22.810430: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-12 15:27:22.810448: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-12 15:27:22.810513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:27:22.810936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:27:22.811282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-12 15:27:22.811417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-12 15:27:22.811429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-12 15:27:22.811436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-12 15:27:22.811518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:27:22.811896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:27:22.812235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-21065\n",
            "I0312 15:27:22.814501 139913100998528 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-21065\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0312 15:27:23.752496 139913100998528 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0312 15:27:23.906537 139913100998528 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 208 images.\n",
            "I0312 15:28:54.017224 139909166556928 coco_evaluation.py:205] Performing evaluation on 208 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0312 15:28:54.018368 139909166556928 coco_tools.py:115] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.02s)\n",
            "I0312 15:28:54.037526 139909166556928 coco_tools.py:137] DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.31s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.008\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.012\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.009\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.011\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.037\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.048\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.048\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.060\n",
            "INFO:tensorflow:Finished evaluation at 2020-03-12-15:28:56\n",
            "I0312 15:28:56.503178 139913100998528 evaluation.py:275] Finished evaluation at 2020-03-12-15:28:56\n",
            "INFO:tensorflow:Saving dict for global step 21065: DetectionBoxes_Precision/mAP = 0.007981217, DetectionBoxes_Precision/mAP (large) = 0.011337664, DetectionBoxes_Precision/mAP (medium) = 7.334067e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.012460866, DetectionBoxes_Precision/mAP@.75IOU = 0.008918246, DetectionBoxes_Recall/AR@1 = 0.0373588, DetectionBoxes_Recall/AR@10 = 0.047527738, DetectionBoxes_Recall/AR@100 = 0.047785137, DetectionBoxes_Recall/AR@100 (large) = 0.06015873, DetectionBoxes_Recall/AR@100 (medium) = 0.000617284, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 21.38847, Loss/localization_loss = 3.3120484, Loss/regularization_loss = 0.26770788, Loss/total_loss = 24.96822, global_step = 21065, learning_rate = 0.004, loss = 24.96822\n",
            "I0312 15:28:56.503418 139913100998528 estimator.py:2049] Saving dict for global step 21065: DetectionBoxes_Precision/mAP = 0.007981217, DetectionBoxes_Precision/mAP (large) = 0.011337664, DetectionBoxes_Precision/mAP (medium) = 7.334067e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.012460866, DetectionBoxes_Precision/mAP@.75IOU = 0.008918246, DetectionBoxes_Recall/AR@1 = 0.0373588, DetectionBoxes_Recall/AR@10 = 0.047527738, DetectionBoxes_Recall/AR@100 = 0.047785137, DetectionBoxes_Recall/AR@100 (large) = 0.06015873, DetectionBoxes_Recall/AR@100 (medium) = 0.000617284, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 21.38847, Loss/localization_loss = 3.3120484, Loss/regularization_loss = 0.26770788, Loss/total_loss = 24.96822, global_step = 21065, learning_rate = 0.004, loss = 24.96822\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 21065: drive/My Drive/training_logs_cls/model.ckpt-21065\n",
            "I0312 15:28:56.512814 139913100998528 estimator.py:2109] Saving 'checkpoint_path' summary for global step 21065: drive/My Drive/training_logs_cls/model.ckpt-21065\n",
            "INFO:tensorflow:global_step/sec: 0.235669\n",
            "I0312 15:29:06.058589 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.235669\n",
            "INFO:tensorflow:loss = 2.9337652, step = 21067 (424.323 sec)\n",
            "I0312 15:29:06.059661 139913100998528 basic_session_run_hooks.py:260] loss = 2.9337652, step = 21067 (424.323 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.306161\n",
            "I0312 15:34:32.684566 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.306161\n",
            "INFO:tensorflow:loss = 2.1806667, step = 21167 (326.626 sec)\n",
            "I0312 15:34:32.686053 139913100998528 basic_session_run_hooks.py:260] loss = 2.1806667, step = 21167 (326.626 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 21219 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "I0312 15:37:17.763926 139913100998528 basic_session_run_hooks.py:606] Saving checkpoints for 21219 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0312 15:37:20.795561 139913100998528 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:37:22.834378 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:37:22.866140 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:37:22.894901 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:37:22.925708 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:37:22.954076 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:37:22.981818 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0312 15:37:24.975849 139913100998528 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-03-12T15:37:24Z\n",
            "I0312 15:37:24.992422 139913100998528 evaluation.py:255] Starting evaluation at 2020-03-12T15:37:24Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0312 15:37:25.397745 139913100998528 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-12 15:37:25.398341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:37:25.398779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-12 15:37:25.398875: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-12 15:37:25.398894: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-12 15:37:25.398913: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-12 15:37:25.398928: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-12 15:37:25.398942: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-12 15:37:25.398960: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-12 15:37:25.398977: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-12 15:37:25.399042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:37:25.399399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:37:25.399736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-12 15:37:25.399808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-12 15:37:25.399820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-12 15:37:25.399827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-12 15:37:25.399910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:37:25.400272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:37:25.400621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-21219\n",
            "I0312 15:37:25.402446 139913100998528 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-21219\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0312 15:37:26.425783 139913100998528 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0312 15:37:26.582887 139913100998528 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 208 images.\n",
            "I0312 15:38:59.062161 139909174949632 coco_evaluation.py:205] Performing evaluation on 208 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0312 15:38:59.063344 139909174949632 coco_tools.py:115] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.02s)\n",
            "I0312 15:38:59.079071 139909174949632 coco_tools.py:137] DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.31s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.013\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.018\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.015\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.017\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.046\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.054\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.054\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.070\n",
            "INFO:tensorflow:Finished evaluation at 2020-03-12-15:39:01\n",
            "I0312 15:39:01.508727 139913100998528 evaluation.py:275] Finished evaluation at 2020-03-12-15:39:01\n",
            "INFO:tensorflow:Saving dict for global step 21219: DetectionBoxes_Precision/mAP = 0.012561552, DetectionBoxes_Precision/mAP (large) = 0.01656988, DetectionBoxes_Precision/mAP (medium) = 0.0, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.017785737, DetectionBoxes_Precision/mAP@.75IOU = 0.015395139, DetectionBoxes_Recall/AR@1 = 0.045534376, DetectionBoxes_Recall/AR@10 = 0.053962473, DetectionBoxes_Recall/AR@100 = 0.054477274, DetectionBoxes_Recall/AR@100 (large) = 0.069880955, DetectionBoxes_Recall/AR@100 (medium) = 0.0, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 21.962137, Loss/localization_loss = 3.3401134, Loss/regularization_loss = 0.2677926, Loss/total_loss = 25.570042, global_step = 21219, learning_rate = 0.004, loss = 25.570042\n",
            "I0312 15:39:01.509001 139913100998528 estimator.py:2049] Saving dict for global step 21219: DetectionBoxes_Precision/mAP = 0.012561552, DetectionBoxes_Precision/mAP (large) = 0.01656988, DetectionBoxes_Precision/mAP (medium) = 0.0, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.017785737, DetectionBoxes_Precision/mAP@.75IOU = 0.015395139, DetectionBoxes_Recall/AR@1 = 0.045534376, DetectionBoxes_Recall/AR@10 = 0.053962473, DetectionBoxes_Recall/AR@100 = 0.054477274, DetectionBoxes_Recall/AR@100 (large) = 0.069880955, DetectionBoxes_Recall/AR@100 (medium) = 0.0, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 21.962137, Loss/localization_loss = 3.3401134, Loss/regularization_loss = 0.2677926, Loss/total_loss = 25.570042, global_step = 21219, learning_rate = 0.004, loss = 25.570042\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 21219: drive/My Drive/training_logs_cls/model.ckpt-21219\n",
            "I0312 15:39:01.518907 139913100998528 estimator.py:2109] Saving 'checkpoint_path' summary for global step 21219: drive/My Drive/training_logs_cls/model.ckpt-21219\n",
            "INFO:tensorflow:global_step/sec: 0.233779\n",
            "I0312 15:41:40.438147 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.233779\n",
            "INFO:tensorflow:loss = 2.3240666, step = 21267 (427.753 sec)\n",
            "I0312 15:41:40.439391 139913100998528 basic_session_run_hooks.py:260] loss = 2.3240666, step = 21267 (427.753 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.307912\n",
            "I0312 15:47:05.206506 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.307912\n",
            "INFO:tensorflow:loss = 2.459699, step = 21367 (324.769 sec)\n",
            "I0312 15:47:05.208024 139913100998528 basic_session_run_hooks.py:260] loss = 2.459699, step = 21367 (324.769 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 21372 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "I0312 15:47:18.797658 139913100998528 basic_session_run_hooks.py:606] Saving checkpoints for 21372 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0312 15:47:21.864536 139913100998528 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:47:23.920698 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:47:23.947942 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:47:23.975300 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:47:24.002358 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:47:24.028531 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:47:24.055045 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0312 15:47:25.511292 139913100998528 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-03-12T15:47:25Z\n",
            "I0312 15:47:25.526501 139913100998528 evaluation.py:255] Starting evaluation at 2020-03-12T15:47:25Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0312 15:47:25.934033 139913100998528 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-12 15:47:25.934723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:47:25.935146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-12 15:47:25.935248: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-12 15:47:25.935269: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-12 15:47:25.935284: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-12 15:47:25.935299: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-12 15:47:25.935313: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-12 15:47:25.935327: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-12 15:47:25.935342: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-12 15:47:25.935398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:47:25.935777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:47:25.936098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-12 15:47:25.936137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-12 15:47:25.936148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-12 15:47:25.936155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-12 15:47:25.936224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:47:25.936578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:47:25.936923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-21372\n",
            "I0312 15:47:25.938517 139913100998528 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-21372\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0312 15:47:26.954710 139913100998528 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0312 15:47:27.080966 139913100998528 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 208 images.\n",
            "I0312 15:49:01.337689 139909174949632 coco_evaluation.py:205] Performing evaluation on 208 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0312 15:49:01.338369 139909174949632 coco_tools.py:115] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.01s)\n",
            "I0312 15:49:01.353460 139909174949632 coco_tools.py:137] DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.34s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.008\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.012\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.008\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.002\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.011\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.049\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.056\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.056\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.040\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.072\n",
            "INFO:tensorflow:Finished evaluation at 2020-03-12-15:49:03\n",
            "I0312 15:49:03.827658 139913100998528 evaluation.py:275] Finished evaluation at 2020-03-12-15:49:03\n",
            "INFO:tensorflow:Saving dict for global step 21372: DetectionBoxes_Precision/mAP = 0.0077901627, DetectionBoxes_Precision/mAP (large) = 0.010805374, DetectionBoxes_Precision/mAP (medium) = 0.0, DetectionBoxes_Precision/mAP (small) = 0.002352941, DetectionBoxes_Precision/mAP@.50IOU = 0.011723307, DetectionBoxes_Precision/mAP@.75IOU = 0.008315993, DetectionBoxes_Recall/AR@1 = 0.048844542, DetectionBoxes_Recall/AR@10 = 0.055675842, DetectionBoxes_Recall/AR@100 = 0.055675842, DetectionBoxes_Recall/AR@100 (large) = 0.07230159, DetectionBoxes_Recall/AR@100 (medium) = 0.0, DetectionBoxes_Recall/AR@100 (small) = 0.04, Loss/classification_loss = 23.170424, Loss/localization_loss = 3.3393657, Loss/regularization_loss = 0.26788357, Loss/total_loss = 26.777676, global_step = 21372, learning_rate = 0.004, loss = 26.777676\n",
            "I0312 15:49:03.827918 139913100998528 estimator.py:2049] Saving dict for global step 21372: DetectionBoxes_Precision/mAP = 0.0077901627, DetectionBoxes_Precision/mAP (large) = 0.010805374, DetectionBoxes_Precision/mAP (medium) = 0.0, DetectionBoxes_Precision/mAP (small) = 0.002352941, DetectionBoxes_Precision/mAP@.50IOU = 0.011723307, DetectionBoxes_Precision/mAP@.75IOU = 0.008315993, DetectionBoxes_Recall/AR@1 = 0.048844542, DetectionBoxes_Recall/AR@10 = 0.055675842, DetectionBoxes_Recall/AR@100 = 0.055675842, DetectionBoxes_Recall/AR@100 (large) = 0.07230159, DetectionBoxes_Recall/AR@100 (medium) = 0.0, DetectionBoxes_Recall/AR@100 (small) = 0.04, Loss/classification_loss = 23.170424, Loss/localization_loss = 3.3393657, Loss/regularization_loss = 0.26788357, Loss/total_loss = 26.777676, global_step = 21372, learning_rate = 0.004, loss = 26.777676\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 21372: drive/My Drive/training_logs_cls/model.ckpt-21372\n",
            "I0312 15:49:03.838855 139913100998528 estimator.py:2109] Saving 'checkpoint_path' summary for global step 21372: drive/My Drive/training_logs_cls/model.ckpt-21372\n",
            "INFO:tensorflow:global_step/sec: 0.233451\n",
            "I0312 15:54:13.561124 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.233451\n",
            "INFO:tensorflow:loss = 2.73875, step = 21467 (428.355 sec)\n",
            "I0312 15:54:13.562546 139913100998528 basic_session_run_hooks.py:260] loss = 2.73875, step = 21467 (428.355 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 21526 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "I0312 15:57:21.251856 139913100998528 basic_session_run_hooks.py:606] Saving checkpoints for 21526 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0312 15:57:24.310259 139913100998528 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:57:26.344134 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:57:26.379960 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:57:26.414373 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:57:26.442850 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:57:26.470590 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 15:57:26.497437 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0312 15:57:28.501595 139913100998528 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-03-12T15:57:28Z\n",
            "I0312 15:57:28.516778 139913100998528 evaluation.py:255] Starting evaluation at 2020-03-12T15:57:28Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0312 15:57:28.923346 139913100998528 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-12 15:57:28.924020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:57:28.924479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-12 15:57:28.924575: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-12 15:57:28.924593: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-12 15:57:28.924633: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-12 15:57:28.924650: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-12 15:57:28.924665: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-12 15:57:28.924679: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-12 15:57:28.924693: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-12 15:57:28.924760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:57:28.925146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:57:28.925505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-12 15:57:28.925687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-12 15:57:28.925701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-12 15:57:28.925710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-12 15:57:28.925809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:57:28.926185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 15:57:28.926522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-21526\n",
            "I0312 15:57:28.928077 139913100998528 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-21526\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0312 15:57:29.900318 139913100998528 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0312 15:57:30.055391 139913100998528 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 208 images.\n",
            "I0312 15:59:04.863526 139909174949632 coco_evaluation.py:205] Performing evaluation on 208 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0312 15:59:04.865115 139909174949632 coco_tools.py:115] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.02s)\n",
            "I0312 15:59:04.881757 139909174949632 coco_tools.py:137] DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.33s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.006\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.010\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.009\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.009\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.036\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.039\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.039\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.003\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.047\n",
            "INFO:tensorflow:Finished evaluation at 2020-03-12-15:59:07\n",
            "I0312 15:59:07.300217 139913100998528 evaluation.py:275] Finished evaluation at 2020-03-12-15:59:07\n",
            "INFO:tensorflow:Saving dict for global step 21526: DetectionBoxes_Precision/mAP = 0.0063959556, DetectionBoxes_Precision/mAP (large) = 0.008981444, DetectionBoxes_Precision/mAP (medium) = 2.1130161e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.010073918, DetectionBoxes_Precision/mAP@.75IOU = 0.008583643, DetectionBoxes_Recall/AR@1 = 0.035789203, DetectionBoxes_Recall/AR@10 = 0.039258167, DetectionBoxes_Recall/AR@100 = 0.039258167, DetectionBoxes_Recall/AR@100 (large) = 0.04736111, DetectionBoxes_Recall/AR@100 (medium) = 0.0034313726, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 22.527851, Loss/localization_loss = 3.3102338, Loss/regularization_loss = 0.2679776, Loss/total_loss = 26.106066, global_step = 21526, learning_rate = 0.004, loss = 26.106066\n",
            "I0312 15:59:07.300490 139913100998528 estimator.py:2049] Saving dict for global step 21526: DetectionBoxes_Precision/mAP = 0.0063959556, DetectionBoxes_Precision/mAP (large) = 0.008981444, DetectionBoxes_Precision/mAP (medium) = 2.1130161e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.010073918, DetectionBoxes_Precision/mAP@.75IOU = 0.008583643, DetectionBoxes_Recall/AR@1 = 0.035789203, DetectionBoxes_Recall/AR@10 = 0.039258167, DetectionBoxes_Recall/AR@100 = 0.039258167, DetectionBoxes_Recall/AR@100 (large) = 0.04736111, DetectionBoxes_Recall/AR@100 (medium) = 0.0034313726, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 22.527851, Loss/localization_loss = 3.3102338, Loss/regularization_loss = 0.2679776, Loss/total_loss = 26.106066, global_step = 21526, learning_rate = 0.004, loss = 26.106066\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 21526: drive/My Drive/training_logs_cls/model.ckpt-21526\n",
            "I0312 15:59:07.311664 139913100998528 estimator.py:2109] Saving 'checkpoint_path' summary for global step 21526: drive/My Drive/training_logs_cls/model.ckpt-21526\n",
            "INFO:tensorflow:global_step/sec: 0.234309\n",
            "I0312 16:01:20.347873 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.234309\n",
            "INFO:tensorflow:loss = 2.6237056, step = 21567 (426.787 sec)\n",
            "I0312 16:01:20.349174 139913100998528 basic_session_run_hooks.py:260] loss = 2.6237056, step = 21567 (426.787 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.310046\n",
            "I0312 16:06:42.881030 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.310046\n",
            "INFO:tensorflow:loss = 2.8622775, step = 21667 (322.533 sec)\n",
            "I0312 16:06:42.882210 139913100998528 basic_session_run_hooks.py:260] loss = 2.8622775, step = 21667 (322.533 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 21680 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "I0312 16:07:21.666000 139913100998528 basic_session_run_hooks.py:606] Saving checkpoints for 21680 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0312 16:07:24.726796 139913100998528 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:07:26.850429 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:07:26.878376 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:07:26.905460 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:07:26.932815 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:07:26.959647 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:07:26.987998 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0312 16:07:28.437228 139913100998528 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-03-12T16:07:28Z\n",
            "I0312 16:07:28.452043 139913100998528 evaluation.py:255] Starting evaluation at 2020-03-12T16:07:28Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0312 16:07:28.843154 139913100998528 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-12 16:07:28.843777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:07:28.844194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-12 16:07:28.844294: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-12 16:07:28.844328: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-12 16:07:28.844358: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-12 16:07:28.844381: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-12 16:07:28.844402: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-12 16:07:28.844427: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-12 16:07:28.844451: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-12 16:07:28.844531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:07:28.844971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:07:28.845307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-12 16:07:28.845355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-12 16:07:28.845369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-12 16:07:28.845379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-12 16:07:28.845466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:07:28.845866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:07:28.846212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-21680\n",
            "I0312 16:07:28.848358 139913100998528 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-21680\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0312 16:07:29.825734 139913100998528 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0312 16:07:29.973958 139913100998528 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 208 images.\n",
            "I0312 16:09:01.917583 139909166556928 coco_evaluation.py:205] Performing evaluation on 208 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0312 16:09:01.918350 139909166556928 coco_tools.py:115] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.01s)\n",
            "I0312 16:09:01.931519 139909166556928 coco_tools.py:137] DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.32s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.007\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.013\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.009\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.041\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.057\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.058\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.074\n",
            "INFO:tensorflow:Finished evaluation at 2020-03-12-16:09:04\n",
            "I0312 16:09:04.362634 139913100998528 evaluation.py:275] Finished evaluation at 2020-03-12-16:09:04\n",
            "INFO:tensorflow:Saving dict for global step 21680: DetectionBoxes_Precision/mAP = 0.007397752, DetectionBoxes_Precision/mAP (large) = 0.01010617, DetectionBoxes_Precision/mAP (medium) = 0.0, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.013498709, DetectionBoxes_Precision/mAP@.75IOU = 0.008860656, DetectionBoxes_Recall/AR@1 = 0.041474767, DetectionBoxes_Recall/AR@10 = 0.057417035, DetectionBoxes_Recall/AR@100 = 0.057674434, DetectionBoxes_Recall/AR@100 (large) = 0.07394841, DetectionBoxes_Recall/AR@100 (medium) = 0.0, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 22.012918, Loss/localization_loss = 3.314467, Loss/regularization_loss = 0.26806748, Loss/total_loss = 25.595457, global_step = 21680, learning_rate = 0.004, loss = 25.595457\n",
            "I0312 16:09:04.362928 139913100998528 estimator.py:2049] Saving dict for global step 21680: DetectionBoxes_Precision/mAP = 0.007397752, DetectionBoxes_Precision/mAP (large) = 0.01010617, DetectionBoxes_Precision/mAP (medium) = 0.0, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.013498709, DetectionBoxes_Precision/mAP@.75IOU = 0.008860656, DetectionBoxes_Recall/AR@1 = 0.041474767, DetectionBoxes_Recall/AR@10 = 0.057417035, DetectionBoxes_Recall/AR@100 = 0.057674434, DetectionBoxes_Recall/AR@100 (large) = 0.07394841, DetectionBoxes_Recall/AR@100 (medium) = 0.0, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 22.012918, Loss/localization_loss = 3.314467, Loss/regularization_loss = 0.26806748, Loss/total_loss = 25.595457, global_step = 21680, learning_rate = 0.004, loss = 25.595457\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 21680: drive/My Drive/training_logs_cls/model.ckpt-21680\n",
            "I0312 16:09:04.372943 139913100998528 estimator.py:2109] Saving 'checkpoint_path' summary for global step 21680: drive/My Drive/training_logs_cls/model.ckpt-21680\n",
            "INFO:tensorflow:global_step/sec: 0.234533\n",
            "I0312 16:13:49.260998 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.234533\n",
            "INFO:tensorflow:loss = 2.0684733, step = 21767 (426.381 sec)\n",
            "I0312 16:13:49.262774 139913100998528 basic_session_run_hooks.py:260] loss = 2.0684733, step = 21767 (426.381 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 21834 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "I0312 16:17:24.776341 139913100998528 basic_session_run_hooks.py:606] Saving checkpoints for 21834 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0312 16:17:28.033671 139913100998528 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:17:30.638832 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:17:30.671331 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:17:30.701417 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:17:30.731743 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:17:30.762691 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:17:30.793651 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0312 16:17:32.323244 139913100998528 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-03-12T16:17:32Z\n",
            "I0312 16:17:32.339985 139913100998528 evaluation.py:255] Starting evaluation at 2020-03-12T16:17:32Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0312 16:17:32.748922 139913100998528 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-12 16:17:32.749661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:17:32.750125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-12 16:17:32.750245: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-12 16:17:32.750268: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-12 16:17:32.750289: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-12 16:17:32.750307: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-12 16:17:32.750324: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-12 16:17:32.750343: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-12 16:17:32.750363: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-12 16:17:32.750440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:17:32.750845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:17:32.751214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-12 16:17:32.751402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-12 16:17:32.751415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-12 16:17:32.751422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-12 16:17:32.751513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:17:32.751915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:17:32.752308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-21834\n",
            "I0312 16:17:32.753903 139913100998528 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-21834\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0312 16:17:33.858715 139913100998528 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0312 16:17:34.012524 139913100998528 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 208 images.\n",
            "I0312 16:19:09.788130 139909183342336 coco_evaluation.py:205] Performing evaluation on 208 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0312 16:19:09.789115 139909183342336 coco_tools.py:115] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.01s)\n",
            "I0312 16:19:09.802699 139909183342336 coco_tools.py:137] DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.25s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.32s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.007\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.011\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.007\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.009\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.039\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.045\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.045\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.056\n",
            "INFO:tensorflow:Finished evaluation at 2020-03-12-16:19:12\n",
            "I0312 16:19:12.276493 139913100998528 evaluation.py:275] Finished evaluation at 2020-03-12-16:19:12\n",
            "INFO:tensorflow:Saving dict for global step 21834: DetectionBoxes_Precision/mAP = 0.0067841643, DetectionBoxes_Precision/mAP (large) = 0.00889297, DetectionBoxes_Precision/mAP (medium) = 0.0, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.011374823, DetectionBoxes_Precision/mAP@.75IOU = 0.0067269756, DetectionBoxes_Recall/AR@1 = 0.039449975, DetectionBoxes_Recall/AR@10 = 0.045248255, DetectionBoxes_Recall/AR@100 = 0.045248255, DetectionBoxes_Recall/AR@100 (large) = 0.05581349, DetectionBoxes_Recall/AR@100 (medium) = 0.0, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 21.417866, Loss/localization_loss = 3.2412715, Loss/regularization_loss = 0.26818058, Loss/total_loss = 24.927315, global_step = 21834, learning_rate = 0.004, loss = 24.927315\n",
            "I0312 16:19:12.276825 139913100998528 estimator.py:2049] Saving dict for global step 21834: DetectionBoxes_Precision/mAP = 0.0067841643, DetectionBoxes_Precision/mAP (large) = 0.00889297, DetectionBoxes_Precision/mAP (medium) = 0.0, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.011374823, DetectionBoxes_Precision/mAP@.75IOU = 0.0067269756, DetectionBoxes_Recall/AR@1 = 0.039449975, DetectionBoxes_Recall/AR@10 = 0.045248255, DetectionBoxes_Recall/AR@100 = 0.045248255, DetectionBoxes_Recall/AR@100 (large) = 0.05581349, DetectionBoxes_Recall/AR@100 (medium) = 0.0, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 21.417866, Loss/localization_loss = 3.2412715, Loss/regularization_loss = 0.26818058, Loss/total_loss = 24.927315, global_step = 21834, learning_rate = 0.004, loss = 24.927315\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 21834: drive/My Drive/training_logs_cls/model.ckpt-21834\n",
            "I0312 16:19:12.289433 139913100998528 estimator.py:2109] Saving 'checkpoint_path' summary for global step 21834: drive/My Drive/training_logs_cls/model.ckpt-21834\n",
            "INFO:tensorflow:global_step/sec: 0.228777\n",
            "I0312 16:21:06.368741 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.228777\n",
            "INFO:tensorflow:loss = 2.6632845, step = 21867 (437.107 sec)\n",
            "I0312 16:21:06.370162 139913100998528 basic_session_run_hooks.py:260] loss = 2.6632845, step = 21867 (437.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.304791\n",
            "I0312 16:26:34.461971 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.304791\n",
            "INFO:tensorflow:loss = 2.8893514, step = 21967 (328.093 sec)\n",
            "I0312 16:26:34.463516 139913100998528 basic_session_run_hooks.py:260] loss = 2.8893514, step = 21967 (328.093 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 21984 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "I0312 16:27:27.300575 139913100998528 basic_session_run_hooks.py:606] Saving checkpoints for 21984 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0312 16:27:30.437587 139913100998528 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:27:32.489171 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:27:32.520394 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:27:32.547802 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:27:32.575879 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:27:32.604070 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:27:32.631309 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0312 16:27:34.121299 139913100998528 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-03-12T16:27:34Z\n",
            "I0312 16:27:34.137437 139913100998528 evaluation.py:255] Starting evaluation at 2020-03-12T16:27:34Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0312 16:27:34.558530 139913100998528 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-12 16:27:34.559221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:27:34.559687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-12 16:27:34.559787: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-12 16:27:34.559815: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-12 16:27:34.559840: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-12 16:27:34.559862: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-12 16:27:34.559884: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-12 16:27:34.559905: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-12 16:27:34.559926: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-12 16:27:34.560007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:27:34.560414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:27:34.560782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-12 16:27:34.560831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-12 16:27:34.560846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-12 16:27:34.560856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-12 16:27:34.560955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:27:34.561352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:27:34.561735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-21984\n",
            "I0312 16:27:34.563703 139913100998528 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-21984\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0312 16:27:35.552541 139913100998528 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0312 16:27:35.694343 139913100998528 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 208 images.\n",
            "I0312 16:29:07.047736 139909174949632 coco_evaluation.py:205] Performing evaluation on 208 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0312 16:29:07.049332 139909174949632 coco_tools.py:115] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.02s)\n",
            "I0312 16:29:07.064668 139909174949632 coco_tools.py:137] DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.24s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.32s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.012\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.015\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.012\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.015\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.038\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.046\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.046\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.055\n",
            "INFO:tensorflow:Finished evaluation at 2020-03-12-16:29:09\n",
            "I0312 16:29:09.628665 139913100998528 evaluation.py:275] Finished evaluation at 2020-03-12-16:29:09\n",
            "INFO:tensorflow:Saving dict for global step 21984: DetectionBoxes_Precision/mAP = 0.011600344, DetectionBoxes_Precision/mAP (large) = 0.014625789, DetectionBoxes_Precision/mAP (medium) = 0.0, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.015453495, DetectionBoxes_Precision/mAP@.75IOU = 0.012245481, DetectionBoxes_Recall/AR@1 = 0.03756635, DetectionBoxes_Recall/AR@10 = 0.045837026, DetectionBoxes_Recall/AR@100 = 0.045965724, DetectionBoxes_Recall/AR@100 (large) = 0.055059522, DetectionBoxes_Recall/AR@100 (medium) = 0.0, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 21.075087, Loss/localization_loss = 3.2064319, Loss/regularization_loss = 0.2682809, Loss/total_loss = 24.5498, global_step = 21984, learning_rate = 0.004, loss = 24.5498\n",
            "I0312 16:29:09.628915 139913100998528 estimator.py:2049] Saving dict for global step 21984: DetectionBoxes_Precision/mAP = 0.011600344, DetectionBoxes_Precision/mAP (large) = 0.014625789, DetectionBoxes_Precision/mAP (medium) = 0.0, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.015453495, DetectionBoxes_Precision/mAP@.75IOU = 0.012245481, DetectionBoxes_Recall/AR@1 = 0.03756635, DetectionBoxes_Recall/AR@10 = 0.045837026, DetectionBoxes_Recall/AR@100 = 0.045965724, DetectionBoxes_Recall/AR@100 (large) = 0.055059522, DetectionBoxes_Recall/AR@100 (medium) = 0.0, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 21.075087, Loss/localization_loss = 3.2064319, Loss/regularization_loss = 0.2682809, Loss/total_loss = 24.5498, global_step = 21984, learning_rate = 0.004, loss = 24.5498\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 21984: drive/My Drive/training_logs_cls/model.ckpt-21984\n",
            "I0312 16:29:09.638876 139913100998528 estimator.py:2109] Saving 'checkpoint_path' summary for global step 21984: drive/My Drive/training_logs_cls/model.ckpt-21984\n",
            "INFO:tensorflow:global_step/sec: 0.234124\n",
            "I0312 16:33:41.586717 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.234124\n",
            "INFO:tensorflow:loss = 2.2383287, step = 22067 (427.125 sec)\n",
            "I0312 16:33:41.588211 139913100998528 basic_session_run_hooks.py:260] loss = 2.2383287, step = 22067 (427.125 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 22138 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "I0312 16:37:29.165192 139913100998528 basic_session_run_hooks.py:606] Saving checkpoints for 22138 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0312 16:37:32.296790 139913100998528 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:37:34.370325 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:37:34.399488 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:37:34.427169 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:37:34.454199 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:37:34.481133 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:37:34.509320 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0312 16:37:35.998471 139913100998528 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-03-12T16:37:36Z\n",
            "I0312 16:37:36.013558 139913100998528 evaluation.py:255] Starting evaluation at 2020-03-12T16:37:36Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0312 16:37:36.413841 139913100998528 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-12 16:37:36.414453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:37:36.414895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-12 16:37:36.414999: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-12 16:37:36.415018: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-12 16:37:36.415035: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-12 16:37:36.415057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-12 16:37:36.415086: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-12 16:37:36.415111: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-12 16:37:36.415132: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-12 16:37:36.415208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:37:36.415610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:37:36.415943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-12 16:37:36.415983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-12 16:37:36.415992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-12 16:37:36.416001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-12 16:37:36.416074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:37:36.416425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:37:36.416768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-22138\n",
            "I0312 16:37:36.418499 139913100998528 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-22138\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0312 16:37:37.375504 139913100998528 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0312 16:37:37.517947 139913100998528 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 208 images.\n",
            "I0312 16:39:08.049029 139909191735040 coco_evaluation.py:205] Performing evaluation on 208 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0312 16:39:08.050126 139909191735040 coco_tools.py:115] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.02s)\n",
            "I0312 16:39:08.066658 139909191735040 coco_tools.py:137] DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.32s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.34s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.017\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.023\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.019\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.020\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.044\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.055\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.055\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.003\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.067\n",
            "INFO:tensorflow:Finished evaluation at 2020-03-12-16:39:10\n",
            "I0312 16:39:10.579357 139913100998528 evaluation.py:275] Finished evaluation at 2020-03-12-16:39:10\n",
            "INFO:tensorflow:Saving dict for global step 22138: DetectionBoxes_Precision/mAP = 0.016744042, DetectionBoxes_Precision/mAP (large) = 0.019695815, DetectionBoxes_Precision/mAP (medium) = 2.3586173e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.02250034, DetectionBoxes_Precision/mAP@.75IOU = 0.019300053, DetectionBoxes_Recall/AR@1 = 0.043840952, DetectionBoxes_Recall/AR@10 = 0.055195954, DetectionBoxes_Recall/AR@100 = 0.055195954, DetectionBoxes_Recall/AR@100 (large) = 0.06710318, DetectionBoxes_Recall/AR@100 (medium) = 0.002705156, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 21.368929, Loss/localization_loss = 3.2234075, Loss/regularization_loss = 0.26838198, Loss/total_loss = 24.860725, global_step = 22138, learning_rate = 0.004, loss = 24.860725\n",
            "I0312 16:39:10.579661 139913100998528 estimator.py:2049] Saving dict for global step 22138: DetectionBoxes_Precision/mAP = 0.016744042, DetectionBoxes_Precision/mAP (large) = 0.019695815, DetectionBoxes_Precision/mAP (medium) = 2.3586173e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.02250034, DetectionBoxes_Precision/mAP@.75IOU = 0.019300053, DetectionBoxes_Recall/AR@1 = 0.043840952, DetectionBoxes_Recall/AR@10 = 0.055195954, DetectionBoxes_Recall/AR@100 = 0.055195954, DetectionBoxes_Recall/AR@100 (large) = 0.06710318, DetectionBoxes_Recall/AR@100 (medium) = 0.002705156, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 21.368929, Loss/localization_loss = 3.2234075, Loss/regularization_loss = 0.26838198, Loss/total_loss = 24.860725, global_step = 22138, learning_rate = 0.004, loss = 24.860725\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 22138: drive/My Drive/training_logs_cls/model.ckpt-22138\n",
            "I0312 16:39:10.590553 139913100998528 estimator.py:2109] Saving 'checkpoint_path' summary for global step 22138: drive/My Drive/training_logs_cls/model.ckpt-22138\n",
            "INFO:tensorflow:global_step/sec: 0.23422\n",
            "I0312 16:40:48.535348 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.23422\n",
            "INFO:tensorflow:loss = 3.3109982, step = 22167 (426.949 sec)\n",
            "I0312 16:40:48.537007 139913100998528 basic_session_run_hooks.py:260] loss = 3.3109982, step = 22167 (426.949 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.307518\n",
            "I0312 16:46:13.719756 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.307518\n",
            "INFO:tensorflow:loss = 2.667026, step = 22267 (325.185 sec)\n",
            "I0312 16:46:13.721675 139913100998528 basic_session_run_hooks.py:260] loss = 2.667026, step = 22267 (325.185 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 22292 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "I0312 16:47:32.329320 139913100998528 basic_session_run_hooks.py:606] Saving checkpoints for 22292 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0312 16:47:35.421593 139913100998528 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:47:37.517969 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:47:37.551129 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:47:37.583841 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:47:37.614120 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:47:37.644280 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:47:37.676132 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0312 16:47:39.186304 139913100998528 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-03-12T16:47:39Z\n",
            "I0312 16:47:39.201335 139913100998528 evaluation.py:255] Starting evaluation at 2020-03-12T16:47:39Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0312 16:47:40.110392 139913100998528 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-12 16:47:40.111232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:47:40.111680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-12 16:47:40.111777: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-12 16:47:40.111797: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-12 16:47:40.111812: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-12 16:47:40.111827: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-12 16:47:40.111842: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-12 16:47:40.111856: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-12 16:47:40.111871: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-12 16:47:40.111936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:47:40.112300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:47:40.112632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-12 16:47:40.112728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-12 16:47:40.112740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-12 16:47:40.112746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-12 16:47:40.112822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:47:40.113186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:47:40.113568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-22292\n",
            "I0312 16:47:40.115833 139913100998528 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-22292\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0312 16:47:41.208610 139913100998528 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0312 16:47:41.362227 139913100998528 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 208 images.\n",
            "I0312 16:49:20.078395 139909166556928 coco_evaluation.py:205] Performing evaluation on 208 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0312 16:49:20.079835 139909166556928 coco_tools.py:115] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.01s)\n",
            "I0312 16:49:20.090753 139909166556928 coco_tools.py:137] DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.31s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.32s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.012\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.017\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.013\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.016\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.045\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.049\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.049\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.060\n",
            "INFO:tensorflow:Finished evaluation at 2020-03-12-16:49:22\n",
            "I0312 16:49:22.588270 139913100998528 evaluation.py:275] Finished evaluation at 2020-03-12-16:49:22\n",
            "INFO:tensorflow:Saving dict for global step 22292: DetectionBoxes_Precision/mAP = 0.012085375, DetectionBoxes_Precision/mAP (large) = 0.0156359, DetectionBoxes_Precision/mAP (medium) = 1.4043958e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.01733949, DetectionBoxes_Precision/mAP@.75IOU = 0.013281815, DetectionBoxes_Recall/AR@1 = 0.044758655, DetectionBoxes_Recall/AR@10 = 0.04939863, DetectionBoxes_Recall/AR@100 = 0.04939863, DetectionBoxes_Recall/AR@100 (large) = 0.060396824, DetectionBoxes_Recall/AR@100 (medium) = 0.000617284, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 23.41167, Loss/localization_loss = 3.35925, Loss/regularization_loss = 0.26850805, Loss/total_loss = 27.039448, global_step = 22292, learning_rate = 0.004, loss = 27.039448\n",
            "I0312 16:49:22.588534 139913100998528 estimator.py:2049] Saving dict for global step 22292: DetectionBoxes_Precision/mAP = 0.012085375, DetectionBoxes_Precision/mAP (large) = 0.0156359, DetectionBoxes_Precision/mAP (medium) = 1.4043958e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.01733949, DetectionBoxes_Precision/mAP@.75IOU = 0.013281815, DetectionBoxes_Recall/AR@1 = 0.044758655, DetectionBoxes_Recall/AR@10 = 0.04939863, DetectionBoxes_Recall/AR@100 = 0.04939863, DetectionBoxes_Recall/AR@100 (large) = 0.060396824, DetectionBoxes_Recall/AR@100 (medium) = 0.000617284, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 23.41167, Loss/localization_loss = 3.35925, Loss/regularization_loss = 0.26850805, Loss/total_loss = 27.039448, global_step = 22292, learning_rate = 0.004, loss = 27.039448\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 22292: drive/My Drive/training_logs_cls/model.ckpt-22292\n",
            "I0312 16:49:22.598678 139913100998528 estimator.py:2109] Saving 'checkpoint_path' summary for global step 22292: drive/My Drive/training_logs_cls/model.ckpt-22292\n",
            "INFO:tensorflow:global_step/sec: 0.230235\n",
            "I0312 16:53:28.058263 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.230235\n",
            "INFO:tensorflow:loss = 2.5738244, step = 22367 (434.338 sec)\n",
            "I0312 16:53:28.059786 139913100998528 basic_session_run_hooks.py:260] loss = 2.5738244, step = 22367 (434.338 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 22445 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "I0312 16:57:35.746479 139913100998528 basic_session_run_hooks.py:606] Saving checkpoints for 22445 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0312 16:57:38.779588 139913100998528 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:57:40.854917 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:57:40.884021 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:57:40.913137 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:57:40.939933 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:57:40.967130 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 16:57:40.994307 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0312 16:57:42.409106 139913100998528 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-03-12T16:57:42Z\n",
            "I0312 16:57:42.426997 139913100998528 evaluation.py:255] Starting evaluation at 2020-03-12T16:57:42Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0312 16:57:42.834042 139913100998528 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-12 16:57:42.834693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:57:42.835138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-12 16:57:42.835230: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-12 16:57:42.835249: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-12 16:57:42.835267: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-12 16:57:42.835282: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-12 16:57:42.835297: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-12 16:57:42.835314: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-12 16:57:42.835332: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-12 16:57:42.835394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:57:42.835779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:57:42.836102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-12 16:57:42.836172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-12 16:57:42.836183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-12 16:57:42.836190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-12 16:57:42.836263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:57:42.836635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 16:57:42.836975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-22445\n",
            "I0312 16:57:42.839004 139913100998528 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-22445\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0312 16:57:43.839154 139913100998528 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0312 16:57:43.989406 139913100998528 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 208 images.\n",
            "I0312 16:59:20.085304 139909183342336 coco_evaluation.py:205] Performing evaluation on 208 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0312 16:59:20.086313 139909183342336 coco_tools.py:115] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.01s)\n",
            "I0312 16:59:20.096457 139909183342336 coco_tools.py:137] DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.34s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.010\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.015\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.011\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.003\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.013\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.036\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.047\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.047\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.020\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.057\n",
            "INFO:tensorflow:Finished evaluation at 2020-03-12-16:59:22\n",
            "I0312 16:59:22.534297 139913100998528 evaluation.py:275] Finished evaluation at 2020-03-12-16:59:22\n",
            "INFO:tensorflow:Saving dict for global step 22445: DetectionBoxes_Precision/mAP = 0.010366649, DetectionBoxes_Precision/mAP (large) = 0.013373008, DetectionBoxes_Precision/mAP (medium) = 1.6303176e-05, DetectionBoxes_Precision/mAP (small) = 0.0028571428, DetectionBoxes_Precision/mAP@.50IOU = 0.015332333, DetectionBoxes_Precision/mAP@.75IOU = 0.011302555, DetectionBoxes_Recall/AR@1 = 0.03632351, DetectionBoxes_Recall/AR@10 = 0.0466724, DetectionBoxes_Recall/AR@100 = 0.0466724, DetectionBoxes_Recall/AR@100 (large) = 0.056765873, DetectionBoxes_Recall/AR@100 (medium) = 0.00110748, DetectionBoxes_Recall/AR@100 (small) = 0.02, Loss/classification_loss = 22.486143, Loss/localization_loss = 3.2272303, Loss/regularization_loss = 0.26859665, Loss/total_loss = 25.981966, global_step = 22445, learning_rate = 0.004, loss = 25.981966\n",
            "I0312 16:59:22.534574 139913100998528 estimator.py:2049] Saving dict for global step 22445: DetectionBoxes_Precision/mAP = 0.010366649, DetectionBoxes_Precision/mAP (large) = 0.013373008, DetectionBoxes_Precision/mAP (medium) = 1.6303176e-05, DetectionBoxes_Precision/mAP (small) = 0.0028571428, DetectionBoxes_Precision/mAP@.50IOU = 0.015332333, DetectionBoxes_Precision/mAP@.75IOU = 0.011302555, DetectionBoxes_Recall/AR@1 = 0.03632351, DetectionBoxes_Recall/AR@10 = 0.0466724, DetectionBoxes_Recall/AR@100 = 0.0466724, DetectionBoxes_Recall/AR@100 (large) = 0.056765873, DetectionBoxes_Recall/AR@100 (medium) = 0.00110748, DetectionBoxes_Recall/AR@100 (small) = 0.02, Loss/classification_loss = 22.486143, Loss/localization_loss = 3.2272303, Loss/regularization_loss = 0.26859665, Loss/total_loss = 25.981966, global_step = 22445, learning_rate = 0.004, loss = 25.981966\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 22445: drive/My Drive/training_logs_cls/model.ckpt-22445\n",
            "I0312 16:59:22.547116 139913100998528 estimator.py:2109] Saving 'checkpoint_path' summary for global step 22445: drive/My Drive/training_logs_cls/model.ckpt-22445\n",
            "INFO:tensorflow:global_step/sec: 0.233576\n",
            "I0312 17:00:36.183637 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.233576\n",
            "INFO:tensorflow:loss = 2.4296455, step = 22467 (428.125 sec)\n",
            "I0312 17:00:36.184872 139913100998528 basic_session_run_hooks.py:260] loss = 2.4296455, step = 22467 (428.125 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.311442\n",
            "I0312 17:05:57.270799 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.311442\n",
            "INFO:tensorflow:loss = 3.01133, step = 22567 (321.087 sec)\n",
            "I0312 17:05:57.272112 139913100998528 basic_session_run_hooks.py:260] loss = 3.01133, step = 22567 (321.087 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 22599 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "I0312 17:07:36.500909 139913100998528 basic_session_run_hooks.py:606] Saving checkpoints for 22599 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0312 17:07:39.528651 139913100998528 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:07:41.557510 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:07:41.585309 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:07:41.612198 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:07:41.639724 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:07:41.667077 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:07:41.694922 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0312 17:07:43.614511 139913100998528 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-03-12T17:07:43Z\n",
            "I0312 17:07:43.629897 139913100998528 evaluation.py:255] Starting evaluation at 2020-03-12T17:07:43Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0312 17:07:44.032729 139913100998528 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-12 17:07:44.033379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:07:44.033826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-12 17:07:44.033917: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-12 17:07:44.033937: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-12 17:07:44.033955: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-12 17:07:44.033973: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-12 17:07:44.033994: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-12 17:07:44.034011: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-12 17:07:44.034028: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-12 17:07:44.034092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:07:44.034453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:07:44.034785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-12 17:07:44.034861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-12 17:07:44.034873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-12 17:07:44.034879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-12 17:07:44.034957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:07:44.035325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:07:44.035703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-22599\n",
            "I0312 17:07:44.037752 139913100998528 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-22599\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0312 17:07:45.009582 139913100998528 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0312 17:07:45.171001 139913100998528 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 208 images.\n",
            "I0312 17:09:24.514752 139909191735040 coco_evaluation.py:205] Performing evaluation on 208 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0312 17:09:24.515880 139909191735040 coco_tools.py:115] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.01s)\n",
            "I0312 17:09:24.524475 139909191735040 coco_tools.py:137] DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.32s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.007\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.010\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.006\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.009\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.044\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.052\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.052\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.064\n",
            "INFO:tensorflow:Finished evaluation at 2020-03-12-17:09:27\n",
            "I0312 17:09:27.021716 139913100998528 evaluation.py:275] Finished evaluation at 2020-03-12-17:09:27\n",
            "INFO:tensorflow:Saving dict for global step 22599: DetectionBoxes_Precision/mAP = 0.0065016057, DetectionBoxes_Precision/mAP (large) = 0.008844316, DetectionBoxes_Precision/mAP (medium) = 1.20012e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.009558197, DetectionBoxes_Precision/mAP@.75IOU = 0.0059267688, DetectionBoxes_Recall/AR@1 = 0.044264175, DetectionBoxes_Recall/AR@10 = 0.052060694, DetectionBoxes_Recall/AR@100 = 0.052060694, DetectionBoxes_Recall/AR@100 (large) = 0.06359127, DetectionBoxes_Recall/AR@100 (medium) = 0.000617284, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 23.904623, Loss/localization_loss = 3.1985574, Loss/regularization_loss = 0.26869604, Loss/total_loss = 27.371878, global_step = 22599, learning_rate = 0.004, loss = 27.371878\n",
            "I0312 17:09:27.022012 139913100998528 estimator.py:2049] Saving dict for global step 22599: DetectionBoxes_Precision/mAP = 0.0065016057, DetectionBoxes_Precision/mAP (large) = 0.008844316, DetectionBoxes_Precision/mAP (medium) = 1.20012e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.009558197, DetectionBoxes_Precision/mAP@.75IOU = 0.0059267688, DetectionBoxes_Recall/AR@1 = 0.044264175, DetectionBoxes_Recall/AR@10 = 0.052060694, DetectionBoxes_Recall/AR@100 = 0.052060694, DetectionBoxes_Recall/AR@100 (large) = 0.06359127, DetectionBoxes_Recall/AR@100 (medium) = 0.000617284, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 23.904623, Loss/localization_loss = 3.1985574, Loss/regularization_loss = 0.26869604, Loss/total_loss = 27.371878, global_step = 22599, learning_rate = 0.004, loss = 27.371878\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 22599: drive/My Drive/training_logs_cls/model.ckpt-22599\n",
            "I0312 17:09:27.035475 139913100998528 estimator.py:2109] Saving 'checkpoint_path' summary for global step 22599: drive/My Drive/training_logs_cls/model.ckpt-22599\n",
            "INFO:tensorflow:global_step/sec: 0.230068\n",
            "I0312 17:13:11.925130 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.230068\n",
            "INFO:tensorflow:loss = 2.851221, step = 22667 (434.655 sec)\n",
            "I0312 17:13:11.926950 139913100998528 basic_session_run_hooks.py:260] loss = 2.851221, step = 22667 (434.655 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 22750 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "I0312 17:17:37.988651 139913100998528 basic_session_run_hooks.py:606] Saving checkpoints for 22750 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0312 17:17:41.083310 139913100998528 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:17:43.143901 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:17:43.172784 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:17:43.205967 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:17:43.242486 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:17:43.269373 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:17:43.296714 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0312 17:17:44.767958 139913100998528 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-03-12T17:17:44Z\n",
            "I0312 17:17:44.783111 139913100998528 evaluation.py:255] Starting evaluation at 2020-03-12T17:17:44Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0312 17:17:45.182271 139913100998528 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-12 17:17:45.182911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:17:45.183335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-12 17:17:45.183423: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-12 17:17:45.183442: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-12 17:17:45.183458: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-12 17:17:45.183473: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-12 17:17:45.183487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-12 17:17:45.183501: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-12 17:17:45.183516: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-12 17:17:45.183576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:17:45.183959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:17:45.184280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-12 17:17:45.184356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-12 17:17:45.184368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-12 17:17:45.184374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-12 17:17:45.184448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:17:45.184827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:17:45.185159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-22750\n",
            "I0312 17:17:45.186782 139913100998528 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-22750\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0312 17:17:46.219876 139913100998528 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0312 17:17:46.370711 139913100998528 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 208 images.\n",
            "I0312 17:19:23.165134 139909191735040 coco_evaluation.py:205] Performing evaluation on 208 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0312 17:19:23.166164 139909191735040 coco_tools.py:115] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.01s)\n",
            "I0312 17:19:23.176326 139909191735040 coco_tools.py:137] DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.28s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.34s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.008\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.012\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.010\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.040\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.041\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.042\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.052\n",
            "INFO:tensorflow:Finished evaluation at 2020-03-12-17:19:25\n",
            "I0312 17:19:25.595656 139913100998528 evaluation.py:275] Finished evaluation at 2020-03-12-17:19:25\n",
            "INFO:tensorflow:Saving dict for global step 22750: DetectionBoxes_Precision/mAP = 0.008164792, DetectionBoxes_Precision/mAP (large) = 0.010218621, DetectionBoxes_Precision/mAP (medium) = 1.50015e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.011977806, DetectionBoxes_Precision/mAP@.75IOU = 0.009919746, DetectionBoxes_Recall/AR@1 = 0.04036632, DetectionBoxes_Recall/AR@10 = 0.041063283, DetectionBoxes_Recall/AR@100 = 0.041964184, DetectionBoxes_Recall/AR@100 (large) = 0.05214286, DetectionBoxes_Recall/AR@100 (medium) = 0.0009803922, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 22.100485, Loss/localization_loss = 3.2988112, Loss/regularization_loss = 0.26879495, Loss/total_loss = 25.668083, global_step = 22750, learning_rate = 0.004, loss = 25.668083\n",
            "I0312 17:19:25.595958 139913100998528 estimator.py:2049] Saving dict for global step 22750: DetectionBoxes_Precision/mAP = 0.008164792, DetectionBoxes_Precision/mAP (large) = 0.010218621, DetectionBoxes_Precision/mAP (medium) = 1.50015e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.011977806, DetectionBoxes_Precision/mAP@.75IOU = 0.009919746, DetectionBoxes_Recall/AR@1 = 0.04036632, DetectionBoxes_Recall/AR@10 = 0.041063283, DetectionBoxes_Recall/AR@100 = 0.041964184, DetectionBoxes_Recall/AR@100 (large) = 0.05214286, DetectionBoxes_Recall/AR@100 (medium) = 0.0009803922, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 22.100485, Loss/localization_loss = 3.2988112, Loss/regularization_loss = 0.26879495, Loss/total_loss = 25.668083, global_step = 22750, learning_rate = 0.004, loss = 25.668083\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 22750: drive/My Drive/training_logs_cls/model.ckpt-22750\n",
            "I0312 17:19:25.606261 139913100998528 estimator.py:2109] Saving 'checkpoint_path' summary for global step 22750: drive/My Drive/training_logs_cls/model.ckpt-22750\n",
            "INFO:tensorflow:global_step/sec: 0.231761\n",
            "I0312 17:20:23.404660 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.231761\n",
            "INFO:tensorflow:loss = 2.4015217, step = 22767 (431.479 sec)\n",
            "I0312 17:20:23.406074 139913100998528 basic_session_run_hooks.py:260] loss = 2.4015217, step = 22767 (431.479 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.311395\n",
            "I0312 17:25:44.540048 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.311395\n",
            "INFO:tensorflow:loss = 2.2405095, step = 22867 (321.135 sec)\n",
            "I0312 17:25:44.541309 139913100998528 basic_session_run_hooks.py:260] loss = 2.2405095, step = 22867 (321.135 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 22904 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "I0312 17:27:38.250318 139913100998528 basic_session_run_hooks.py:606] Saving checkpoints for 22904 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0312 17:27:41.309117 139913100998528 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:27:43.807942 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:27:43.835341 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:27:43.861976 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:27:43.888676 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:27:43.916053 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:27:43.942657 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0312 17:27:45.393880 139913100998528 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-03-12T17:27:45Z\n",
            "I0312 17:27:45.408715 139913100998528 evaluation.py:255] Starting evaluation at 2020-03-12T17:27:45Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0312 17:27:45.796974 139913100998528 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-12 17:27:45.797543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:27:45.797987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-12 17:27:45.798073: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-12 17:27:45.798092: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-12 17:27:45.798108: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-12 17:27:45.798123: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-12 17:27:45.798138: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-12 17:27:45.798153: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-12 17:27:45.798167: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-12 17:27:45.798228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:27:45.798585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:27:45.798930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-12 17:27:45.798969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-12 17:27:45.798980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-12 17:27:45.798987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-12 17:27:45.799058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:27:45.799409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:27:45.799754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-22904\n",
            "I0312 17:27:45.806215 139913100998528 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-22904\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0312 17:27:46.750372 139913100998528 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0312 17:27:46.889832 139913100998528 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 208 images.\n",
            "I0312 17:29:20.819405 139909174949632 coco_evaluation.py:205] Performing evaluation on 208 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0312 17:29:20.820502 139909174949632 coco_tools.py:115] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.01s)\n",
            "I0312 17:29:20.835597 139909174949632 coco_tools.py:137] DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.20s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.32s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.007\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.011\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.009\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.044\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.045\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.045\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.004\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.055\n",
            "INFO:tensorflow:Finished evaluation at 2020-03-12-17:29:23\n",
            "I0312 17:29:23.291297 139913100998528 evaluation.py:275] Finished evaluation at 2020-03-12-17:29:23\n",
            "INFO:tensorflow:Saving dict for global step 22904: DetectionBoxes_Precision/mAP = 0.007368447, DetectionBoxes_Precision/mAP (large) = 0.009916229, DetectionBoxes_Precision/mAP (medium) = 5.848686e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.010749904, DetectionBoxes_Precision/mAP@.75IOU = 0.00919086, DetectionBoxes_Recall/AR@1 = 0.04424819, DetectionBoxes_Recall/AR@10 = 0.04489169, DetectionBoxes_Recall/AR@100 = 0.04489169, DetectionBoxes_Recall/AR@100 (large) = 0.05531746, DetectionBoxes_Recall/AR@100 (medium) = 0.0043209875, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 23.614632, Loss/localization_loss = 3.4162102, Loss/regularization_loss = 0.26889715, Loss/total_loss = 27.299723, global_step = 22904, learning_rate = 0.004, loss = 27.299723\n",
            "I0312 17:29:23.291563 139913100998528 estimator.py:2049] Saving dict for global step 22904: DetectionBoxes_Precision/mAP = 0.007368447, DetectionBoxes_Precision/mAP (large) = 0.009916229, DetectionBoxes_Precision/mAP (medium) = 5.848686e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.010749904, DetectionBoxes_Precision/mAP@.75IOU = 0.00919086, DetectionBoxes_Recall/AR@1 = 0.04424819, DetectionBoxes_Recall/AR@10 = 0.04489169, DetectionBoxes_Recall/AR@100 = 0.04489169, DetectionBoxes_Recall/AR@100 (large) = 0.05531746, DetectionBoxes_Recall/AR@100 (medium) = 0.0043209875, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 23.614632, Loss/localization_loss = 3.4162102, Loss/regularization_loss = 0.26889715, Loss/total_loss = 27.299723, global_step = 22904, learning_rate = 0.004, loss = 27.299723\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 22904: drive/My Drive/training_logs_cls/model.ckpt-22904\n",
            "I0312 17:29:23.302850 139913100998528 estimator.py:2109] Saving 'checkpoint_path' summary for global step 22904: drive/My Drive/training_logs_cls/model.ckpt-22904\n",
            "INFO:tensorflow:global_step/sec: 0.235889\n",
            "I0312 17:32:48.467758 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.235889\n",
            "INFO:tensorflow:loss = 2.4567442, step = 22967 (423.928 sec)\n",
            "I0312 17:32:48.469241 139913100998528 basic_session_run_hooks.py:260] loss = 2.4567442, step = 22967 (423.928 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 23059 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "I0312 17:37:40.216619 139913100998528 basic_session_run_hooks.py:606] Saving checkpoints for 23059 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0312 17:37:43.204845 139913100998528 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:37:45.229728 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:37:45.259688 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:37:45.289723 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:37:45.318234 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:37:45.345378 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:37:45.374017 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0312 17:37:46.831147 139913100998528 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-03-12T17:37:46Z\n",
            "I0312 17:37:46.846007 139913100998528 evaluation.py:255] Starting evaluation at 2020-03-12T17:37:46Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0312 17:37:47.263830 139913100998528 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-12 17:37:47.264575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:37:47.265098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-12 17:37:47.265221: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-12 17:37:47.265253: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-12 17:37:47.265282: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-12 17:37:47.265304: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-12 17:37:47.265327: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-12 17:37:47.265352: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-12 17:37:47.265378: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-12 17:37:47.265463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:37:47.265955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:37:47.266330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-12 17:37:47.266445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-12 17:37:47.266461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-12 17:37:47.266471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-12 17:37:47.266574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:37:47.267023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:37:47.267388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-23059\n",
            "I0312 17:37:47.269880 139913100998528 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-23059\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0312 17:37:48.314241 139913100998528 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0312 17:37:48.492047 139913100998528 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 208 images.\n",
            "I0312 17:39:16.412316 139909191735040 coco_evaluation.py:205] Performing evaluation on 208 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0312 17:39:16.414931 139909191735040 coco_tools.py:115] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.01s)\n",
            "I0312 17:39:16.429502 139909191735040 coco_tools.py:137] DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.21s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.31s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.009\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.013\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.011\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.013\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.056\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.056\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.056\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.070\n",
            "INFO:tensorflow:Finished evaluation at 2020-03-12-17:39:18\n",
            "I0312 17:39:18.925396 139913100998528 evaluation.py:275] Finished evaluation at 2020-03-12-17:39:18\n",
            "INFO:tensorflow:Saving dict for global step 23059: DetectionBoxes_Precision/mAP = 0.008877609, DetectionBoxes_Precision/mAP (large) = 0.012858679, DetectionBoxes_Precision/mAP (medium) = 3.4740315e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.013118473, DetectionBoxes_Precision/mAP@.75IOU = 0.0114299385, DetectionBoxes_Recall/AR@1 = 0.056153085, DetectionBoxes_Recall/AR@10 = 0.056153085, DetectionBoxes_Recall/AR@100 = 0.056153085, DetectionBoxes_Recall/AR@100 (large) = 0.06952381, DetectionBoxes_Recall/AR@100 (medium) = 0.000617284, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 21.703957, Loss/localization_loss = 3.441151, Loss/regularization_loss = 0.2689923, Loss/total_loss = 25.414097, global_step = 23059, learning_rate = 0.004, loss = 25.414097\n",
            "I0312 17:39:18.925699 139913100998528 estimator.py:2049] Saving dict for global step 23059: DetectionBoxes_Precision/mAP = 0.008877609, DetectionBoxes_Precision/mAP (large) = 0.012858679, DetectionBoxes_Precision/mAP (medium) = 3.4740315e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.013118473, DetectionBoxes_Precision/mAP@.75IOU = 0.0114299385, DetectionBoxes_Recall/AR@1 = 0.056153085, DetectionBoxes_Recall/AR@10 = 0.056153085, DetectionBoxes_Recall/AR@100 = 0.056153085, DetectionBoxes_Recall/AR@100 (large) = 0.06952381, DetectionBoxes_Recall/AR@100 (medium) = 0.000617284, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 21.703957, Loss/localization_loss = 3.441151, Loss/regularization_loss = 0.2689923, Loss/total_loss = 25.414097, global_step = 23059, learning_rate = 0.004, loss = 25.414097\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 23059: drive/My Drive/training_logs_cls/model.ckpt-23059\n",
            "I0312 17:39:18.936257 139913100998528 estimator.py:2109] Saving 'checkpoint_path' summary for global step 23059: drive/My Drive/training_logs_cls/model.ckpt-23059\n",
            "INFO:tensorflow:global_step/sec: 0.237963\n",
            "I0312 17:39:48.701695 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.237963\n",
            "INFO:tensorflow:loss = 1.8826436, step = 23067 (420.234 sec)\n",
            "I0312 17:39:48.702775 139913100998528 basic_session_run_hooks.py:260] loss = 1.8826436, step = 23067 (420.234 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.311476\n",
            "I0312 17:45:09.753941 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.311476\n",
            "INFO:tensorflow:loss = 2.9236555, step = 23167 (321.053 sec)\n",
            "I0312 17:45:09.755746 139913100998528 basic_session_run_hooks.py:260] loss = 2.9236555, step = 23167 (321.053 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 23215 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "I0312 17:47:40.665878 139913100998528 basic_session_run_hooks.py:606] Saving checkpoints for 23215 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0312 17:47:43.755570 139913100998528 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:47:45.816029 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:47:45.847115 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:47:45.875720 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:47:45.911941 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:47:45.939570 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:47:45.969723 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0312 17:47:47.485239 139913100998528 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-03-12T17:47:47Z\n",
            "I0312 17:47:47.500523 139913100998528 evaluation.py:255] Starting evaluation at 2020-03-12T17:47:47Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0312 17:47:47.919866 139913100998528 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-12 17:47:47.920575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:47:47.921009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-12 17:47:47.921107: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-12 17:47:47.921140: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-12 17:47:47.921167: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-12 17:47:47.921189: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-12 17:47:47.921215: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-12 17:47:47.921235: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-12 17:47:47.921256: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-12 17:47:47.921336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:47:47.921751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:47:47.922085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-12 17:47:47.922170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-12 17:47:47.922186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-12 17:47:47.922195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-12 17:47:47.922282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:47:47.922681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:47:47.923169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-23215\n",
            "I0312 17:47:47.925798 139913100998528 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-23215\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0312 17:47:48.928343 139913100998528 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0312 17:47:49.070698 139913100998528 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 208 images.\n",
            "I0312 17:49:18.473376 139909191735040 coco_evaluation.py:205] Performing evaluation on 208 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0312 17:49:18.479973 139909191735040 coco_tools.py:115] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.02s)\n",
            "I0312 17:49:18.499003 139909191735040 coco_tools.py:137] DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.31s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.009\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.016\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.010\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.013\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.050\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.052\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.052\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.002\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.067\n",
            "INFO:tensorflow:Finished evaluation at 2020-03-12-17:49:20\n",
            "I0312 17:49:20.957522 139913100998528 evaluation.py:275] Finished evaluation at 2020-03-12-17:49:20\n",
            "INFO:tensorflow:Saving dict for global step 23215: DetectionBoxes_Precision/mAP = 0.009432061, DetectionBoxes_Precision/mAP (large) = 0.01304384, DetectionBoxes_Precision/mAP (medium) = 8.0008e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.015731934, DetectionBoxes_Precision/mAP@.75IOU = 0.010084918, DetectionBoxes_Recall/AR@1 = 0.050252657, DetectionBoxes_Recall/AR@10 = 0.05157134, DetectionBoxes_Recall/AR@100 = 0.05170004, DetectionBoxes_Recall/AR@100 (large) = 0.06744047, DetectionBoxes_Recall/AR@100 (medium) = 0.002469136, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 22.57304, Loss/localization_loss = 3.414337, Loss/regularization_loss = 0.26908985, Loss/total_loss = 26.256468, global_step = 23215, learning_rate = 0.004, loss = 26.256468\n",
            "I0312 17:49:20.957816 139913100998528 estimator.py:2049] Saving dict for global step 23215: DetectionBoxes_Precision/mAP = 0.009432061, DetectionBoxes_Precision/mAP (large) = 0.01304384, DetectionBoxes_Precision/mAP (medium) = 8.0008e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.015731934, DetectionBoxes_Precision/mAP@.75IOU = 0.010084918, DetectionBoxes_Recall/AR@1 = 0.050252657, DetectionBoxes_Recall/AR@10 = 0.05157134, DetectionBoxes_Recall/AR@100 = 0.05170004, DetectionBoxes_Recall/AR@100 (large) = 0.06744047, DetectionBoxes_Recall/AR@100 (medium) = 0.002469136, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 22.57304, Loss/localization_loss = 3.414337, Loss/regularization_loss = 0.26908985, Loss/total_loss = 26.256468, global_step = 23215, learning_rate = 0.004, loss = 26.256468\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 23215: drive/My Drive/training_logs_cls/model.ckpt-23215\n",
            "I0312 17:49:20.967540 139913100998528 estimator.py:2109] Saving 'checkpoint_path' summary for global step 23215: drive/My Drive/training_logs_cls/model.ckpt-23215\n",
            "INFO:tensorflow:global_step/sec: 0.23791\n",
            "I0312 17:52:10.081541 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.23791\n",
            "INFO:tensorflow:loss = 2.441215, step = 23267 (420.327 sec)\n",
            "I0312 17:52:10.083122 139913100998528 basic_session_run_hooks.py:260] loss = 2.441215, step = 23267 (420.327 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.313348\n",
            "I0312 17:57:29.215032 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.313348\n",
            "INFO:tensorflow:loss = 2.5939784, step = 23367 (319.133 sec)\n",
            "I0312 17:57:29.216438 139913100998528 basic_session_run_hooks.py:260] loss = 2.5939784, step = 23367 (319.133 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 23372 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "I0312 17:57:42.184771 139913100998528 basic_session_run_hooks.py:606] Saving checkpoints for 23372 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0312 17:57:45.308402 139913100998528 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:57:47.357110 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:57:47.386695 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:57:47.414325 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:57:47.441409 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:57:47.468523 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 17:57:47.495131 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0312 17:57:49.006111 139913100998528 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-03-12T17:57:49Z\n",
            "I0312 17:57:49.021341 139913100998528 evaluation.py:255] Starting evaluation at 2020-03-12T17:57:49Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0312 17:57:49.909507 139913100998528 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-12 17:57:49.910121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:57:49.910542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-12 17:57:49.910663: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-12 17:57:49.910698: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-12 17:57:49.910719: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-12 17:57:49.910734: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-12 17:57:49.910749: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-12 17:57:49.910767: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-12 17:57:49.910784: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-12 17:57:49.910850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:57:49.911216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:57:49.911535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-12 17:57:49.911585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-12 17:57:49.911612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-12 17:57:49.911624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-12 17:57:49.911723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:57:49.912086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 17:57:49.912416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-23372\n",
            "I0312 17:57:49.913890 139913100998528 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-23372\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0312 17:57:50.854942 139913100998528 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0312 17:57:51.005767 139913100998528 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 208 images.\n",
            "I0312 17:59:13.719775 139909166556928 coco_evaluation.py:205] Performing evaluation on 208 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0312 17:59:13.724807 139909166556928 coco_tools.py:115] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.01s)\n",
            "I0312 17:59:13.739439 139909166556928 coco_tools.py:137] DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.20s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.33s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.015\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.022\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.015\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.019\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.046\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.050\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.050\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.067\n",
            "INFO:tensorflow:Finished evaluation at 2020-03-12-17:59:16\n",
            "I0312 17:59:16.216894 139913100998528 evaluation.py:275] Finished evaluation at 2020-03-12-17:59:16\n",
            "INFO:tensorflow:Saving dict for global step 23372: DetectionBoxes_Precision/mAP = 0.0152179375, DetectionBoxes_Precision/mAP (large) = 0.019123351, DetectionBoxes_Precision/mAP (medium) = 2.1641508e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.021596683, DetectionBoxes_Precision/mAP@.75IOU = 0.015325442, DetectionBoxes_Recall/AR@1 = 0.046246428, DetectionBoxes_Recall/AR@10 = 0.04967777, DetectionBoxes_Recall/AR@100 = 0.04967777, DetectionBoxes_Recall/AR@100 (large) = 0.067281745, DetectionBoxes_Recall/AR@100 (medium) = 0.001234568, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 21.166231, Loss/localization_loss = 3.4714694, Loss/regularization_loss = 0.26918685, Loss/total_loss = 24.9069, global_step = 23372, learning_rate = 0.004, loss = 24.9069\n",
            "I0312 17:59:16.217145 139913100998528 estimator.py:2049] Saving dict for global step 23372: DetectionBoxes_Precision/mAP = 0.0152179375, DetectionBoxes_Precision/mAP (large) = 0.019123351, DetectionBoxes_Precision/mAP (medium) = 2.1641508e-05, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.021596683, DetectionBoxes_Precision/mAP@.75IOU = 0.015325442, DetectionBoxes_Recall/AR@1 = 0.046246428, DetectionBoxes_Recall/AR@10 = 0.04967777, DetectionBoxes_Recall/AR@100 = 0.04967777, DetectionBoxes_Recall/AR@100 (large) = 0.067281745, DetectionBoxes_Recall/AR@100 (medium) = 0.001234568, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 21.166231, Loss/localization_loss = 3.4714694, Loss/regularization_loss = 0.26918685, Loss/total_loss = 24.9069, global_step = 23372, learning_rate = 0.004, loss = 24.9069\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 23372: drive/My Drive/training_logs_cls/model.ckpt-23372\n",
            "I0312 17:59:16.228406 139913100998528 estimator.py:2109] Saving 'checkpoint_path' summary for global step 23372: drive/My Drive/training_logs_cls/model.ckpt-23372\n",
            "INFO:tensorflow:global_step/sec: 0.24106\n",
            "I0312 18:04:24.049860 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.24106\n",
            "INFO:tensorflow:loss = 2.342478, step = 23467 (414.835 sec)\n",
            "I0312 18:04:24.051190 139913100998528 basic_session_run_hooks.py:260] loss = 2.342478, step = 23467 (414.835 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 23530 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "I0312 18:07:43.228281 139913100998528 basic_session_run_hooks.py:606] Saving checkpoints for 23530 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0312 18:07:46.265761 139913100998528 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 18:07:48.344259 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 18:07:48.373339 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 18:07:48.402238 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 18:07:48.436574 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 18:07:48.470697 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 18:07:48.499570 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0312 18:07:49.949703 139913100998528 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-03-12T18:07:49Z\n",
            "I0312 18:07:49.964759 139913100998528 evaluation.py:255] Starting evaluation at 2020-03-12T18:07:49Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0312 18:07:50.370478 139913100998528 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-12 18:07:50.371288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 18:07:50.371766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-12 18:07:50.371865: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-12 18:07:50.371899: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-12 18:07:50.371924: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-12 18:07:50.371952: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-12 18:07:50.371976: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-12 18:07:50.371998: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-12 18:07:50.372020: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-12 18:07:50.372117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 18:07:50.372514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 18:07:50.372868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-12 18:07:50.373080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-12 18:07:50.373097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-12 18:07:50.373106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-12 18:07:50.373207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 18:07:50.373612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 18:07:50.373964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-23530\n",
            "I0312 18:07:50.376235 139913100998528 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-23530\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0312 18:07:51.380454 139913100998528 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0312 18:07:51.515410 139913100998528 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 208 images.\n",
            "I0312 18:09:28.744866 139909183342336 coco_evaluation.py:205] Performing evaluation on 208 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0312 18:09:28.745941 139909183342336 coco_tools.py:115] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.02s)\n",
            "I0312 18:09:28.764352 139909183342336 coco_tools.py:137] DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.22s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.32s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.012\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.017\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.016\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.015\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.051\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.051\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.051\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.061\n",
            "INFO:tensorflow:Finished evaluation at 2020-03-12-18:09:31\n",
            "I0312 18:09:31.168919 139913100998528 evaluation.py:275] Finished evaluation at 2020-03-12-18:09:31\n",
            "INFO:tensorflow:Saving dict for global step 23530: DetectionBoxes_Precision/mAP = 0.011837952, DetectionBoxes_Precision/mAP (large) = 0.01474174, DetectionBoxes_Precision/mAP (medium) = 0.0, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.017495051, DetectionBoxes_Precision/mAP@.75IOU = 0.015786469, DetectionBoxes_Recall/AR@1 = 0.050831676, DetectionBoxes_Recall/AR@10 = 0.050831676, DetectionBoxes_Recall/AR@100 = 0.050831676, DetectionBoxes_Recall/AR@100 (large) = 0.06081349, DetectionBoxes_Recall/AR@100 (medium) = 0.0, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 24.226126, Loss/localization_loss = 3.3580327, Loss/regularization_loss = 0.26927102, Loss/total_loss = 27.853441, global_step = 23530, learning_rate = 0.004, loss = 27.853441\n",
            "I0312 18:09:31.169157 139913100998528 estimator.py:2049] Saving dict for global step 23530: DetectionBoxes_Precision/mAP = 0.011837952, DetectionBoxes_Precision/mAP (large) = 0.01474174, DetectionBoxes_Precision/mAP (medium) = 0.0, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.017495051, DetectionBoxes_Precision/mAP@.75IOU = 0.015786469, DetectionBoxes_Recall/AR@1 = 0.050831676, DetectionBoxes_Recall/AR@10 = 0.050831676, DetectionBoxes_Recall/AR@100 = 0.050831676, DetectionBoxes_Recall/AR@100 (large) = 0.06081349, DetectionBoxes_Recall/AR@100 (medium) = 0.0, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 24.226126, Loss/localization_loss = 3.3580327, Loss/regularization_loss = 0.26927102, Loss/total_loss = 27.853441, global_step = 23530, learning_rate = 0.004, loss = 27.853441\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 23530: drive/My Drive/training_logs_cls/model.ckpt-23530\n",
            "I0312 18:09:31.178668 139913100998528 estimator.py:2109] Saving 'checkpoint_path' summary for global step 23530: drive/My Drive/training_logs_cls/model.ckpt-23530\n",
            "INFO:tensorflow:global_step/sec: 0.233076\n",
            "I0312 18:11:33.093566 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.233076\n",
            "INFO:tensorflow:loss = 2.4082446, step = 23567 (429.044 sec)\n",
            "I0312 18:11:33.094890 139913100998528 basic_session_run_hooks.py:260] loss = 2.4082446, step = 23567 (429.044 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.311752\n",
            "I0312 18:16:53.861157 139913100998528 basic_session_run_hooks.py:692] global_step/sec: 0.311752\n",
            "INFO:tensorflow:loss = 2.590208, step = 23667 (320.768 sec)\n",
            "I0312 18:16:53.862650 139913100998528 basic_session_run_hooks.py:260] loss = 2.590208, step = 23667 (320.768 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 23684 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "I0312 18:17:44.957869 139913100998528 basic_session_run_hooks.py:606] Saving checkpoints for 23684 into drive/My Drive/training_logs_cls/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0312 18:17:47.966323 139913100998528 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 18:17:50.066237 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 18:17:50.096547 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 18:17:50.123476 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 18:17:50.151808 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 18:17:50.181185 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0312 18:17:50.208894 139913100998528 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0312 18:17:52.110399 139913100998528 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-03-12T18:17:52Z\n",
            "I0312 18:17:52.131118 139913100998528 evaluation.py:255] Starting evaluation at 2020-03-12T18:17:52Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0312 18:17:52.528171 139913100998528 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-12 18:17:52.528859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 18:17:52.529304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-12 18:17:52.529413: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-03-12 18:17:52.529431: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-03-12 18:17:52.529449: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-03-12 18:17:52.529463: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-03-12 18:17:52.529476: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-03-12 18:17:52.529492: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-03-12 18:17:52.529509: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-12 18:17:52.529579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 18:17:52.530015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 18:17:52.530348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-12 18:17:52.530388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-12 18:17:52.530397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-12 18:17:52.530406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-12 18:17:52.530473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 18:17:52.530856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-12 18:17:52.531189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-23684\n",
            "I0312 18:17:52.533128 139913100998528 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-23684\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0312 18:17:53.585240 139913100998528 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0312 18:17:53.738861 139913100998528 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 208 images.\n",
            "I0312 18:19:30.551671 139909166556928 coco_evaluation.py:205] Performing evaluation on 208 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0312 18:19:30.552558 139909166556928 coco_tools.py:115] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.01s)\n",
            "I0312 18:19:30.562165 139909166556928 coco_tools.py:137] DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.31s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.016\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.022\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.019\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.019\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.047\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.048\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.048\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.059\n",
            "INFO:tensorflow:Finished evaluation at 2020-03-12-18:19:33\n",
            "I0312 18:19:33.130712 139913100998528 evaluation.py:275] Finished evaluation at 2020-03-12-18:19:33\n",
            "INFO:tensorflow:Saving dict for global step 23684: DetectionBoxes_Precision/mAP = 0.015575172, DetectionBoxes_Precision/mAP (large) = 0.019210791, DetectionBoxes_Precision/mAP (medium) = 0.0, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.021583768, DetectionBoxes_Precision/mAP@.75IOU = 0.01932158, DetectionBoxes_Recall/AR@1 = 0.0474115, DetectionBoxes_Recall/AR@10 = 0.048055, DetectionBoxes_Recall/AR@100 = 0.048312403, DetectionBoxes_Recall/AR@100 (large) = 0.05906746, DetectionBoxes_Recall/AR@100 (medium) = 0.0, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 24.23339, Loss/localization_loss = 3.1900775, Loss/regularization_loss = 0.26935664, Loss/total_loss = 27.69281, global_step = 23684, learning_rate = 0.004, loss = 27.69281\n",
            "I0312 18:19:33.130958 139913100998528 estimator.py:2049] Saving dict for global step 23684: DetectionBoxes_Precision/mAP = 0.015575172, DetectionBoxes_Precision/mAP (large) = 0.019210791, DetectionBoxes_Precision/mAP (medium) = 0.0, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.021583768, DetectionBoxes_Precision/mAP@.75IOU = 0.01932158, DetectionBoxes_Recall/AR@1 = 0.0474115, DetectionBoxes_Recall/AR@10 = 0.048055, DetectionBoxes_Recall/AR@100 = 0.048312403, DetectionBoxes_Recall/AR@100 (large) = 0.05906746, DetectionBoxes_Recall/AR@100 (medium) = 0.0, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 24.23339, Loss/localization_loss = 3.1900775, Loss/regularization_loss = 0.26935664, Loss/total_loss = 27.69281, global_step = 23684, learning_rate = 0.004, loss = 27.69281\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 23684: drive/My Drive/training_logs_cls/model.ckpt-23684\n",
            "I0312 18:19:33.142366 139913100998528 estimator.py:2109] Saving 'checkpoint_path' summary for global step 23684: drive/My Drive/training_logs_cls/model.ckpt-23684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TA3Ji_aCZSHI",
        "colab": {}
      },
      "source": [
        "# !cp -r training_logs/ drive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NfZw_KuJbcoe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c9636c12-29bf-4e48-9bb4-209d6042664c"
      },
      "source": [
        "# %cd /content\n",
        "# !mkdir trained_model\n",
        "# !rm -rf trained_model/saved_model/*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GFTp9s_Va7oz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b077e629-85c8-4426-e7aa-7362961d0386"
      },
      "source": [
        "%cd /content\n",
        "!python /content/models/research/object_detection/export_inference_graph.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path=garbage_detection/pipeline.config \\\n",
        "    --output_directory=trained_model \\\n",
        "    --trained_checkpoint_prefix=drive/My\\ Drive/training_logs_cls/model.ckpt-25431"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W0604 20:18:24.666207 139959732991872 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0604 20:18:27.686969 139959732991872 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0604 20:18:27.737869 139959732991872 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0604 20:18:27.786223 139959732991872 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0604 20:18:27.834598 139959732991872 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0604 20:18:27.881690 139959732991872 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0604 20:18:27.929511 139959732991872 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/core/post_processing.py:583: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0604 20:18:28.274937 139959732991872 deprecation.py:323] From /content/models/research/object_detection/core/post_processing.py:583: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:400: get_or_create_global_step (from tf_slim.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "W0604 20:18:28.961889 139959732991872 deprecation.py:323] From /content/models/research/object_detection/exporter.py:400: get_or_create_global_step (from tf_slim.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:555: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
            "Instructions for updating:\n",
            "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
            "W0604 20:18:28.966439 139959732991872 deprecation.py:323] From /content/models/research/object_detection/exporter.py:555: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
            "Instructions for updating:\n",
            "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
            "W0604 20:18:28.967943 139959732991872 deprecation.py:323] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
            "149 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              0\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   name\n",
            "-account_type_regexes       _trainable_variables\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     params\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "param: Number of parameters (in the Variable).\n",
            "\n",
            "Profile:\n",
            "node name | # parameters\n",
            "_TFProfRoot (--/4.61m params)\n",
            "  BoxPredictor_0 (--/186.66k params)\n",
            "    BoxPredictor_0/BoxEncodingPredictor (--/62.22k params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor/weights (3x3x576x12, 62.21k/62.21k params)\n",
            "    BoxPredictor_0/ClassPredictor (--/124.44k params)\n",
            "      BoxPredictor_0/ClassPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_0/ClassPredictor/weights (3x3x576x24, 124.42k/124.42k params)\n",
            "  BoxPredictor_1 (--/829.51k params)\n",
            "    BoxPredictor_1/BoxEncodingPredictor (--/276.50k params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor/weights (3x3x1280x24, 276.48k/276.48k params)\n",
            "    BoxPredictor_1/ClassPredictor (--/553.01k params)\n",
            "      BoxPredictor_1/ClassPredictor/biases (48, 48/48 params)\n",
            "      BoxPredictor_1/ClassPredictor/weights (3x3x1280x48, 552.96k/552.96k params)\n",
            "  BoxPredictor_2 (--/331.85k params)\n",
            "    BoxPredictor_2/BoxEncodingPredictor (--/110.62k params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor/weights (3x3x512x24, 110.59k/110.59k params)\n",
            "    BoxPredictor_2/ClassPredictor (--/221.23k params)\n",
            "      BoxPredictor_2/ClassPredictor/biases (48, 48/48 params)\n",
            "      BoxPredictor_2/ClassPredictor/weights (3x3x512x48, 221.18k/221.18k params)\n",
            "  BoxPredictor_3 (--/165.96k params)\n",
            "    BoxPredictor_3/BoxEncodingPredictor (--/55.32k params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor/weights (3x3x256x24, 55.30k/55.30k params)\n",
            "    BoxPredictor_3/ClassPredictor (--/110.64k params)\n",
            "      BoxPredictor_3/ClassPredictor/biases (48, 48/48 params)\n",
            "      BoxPredictor_3/ClassPredictor/weights (3x3x256x48, 110.59k/110.59k params)\n",
            "  BoxPredictor_4 (--/165.96k params)\n",
            "    BoxPredictor_4/BoxEncodingPredictor (--/55.32k params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor/weights (3x3x256x24, 55.30k/55.30k params)\n",
            "    BoxPredictor_4/ClassPredictor (--/110.64k params)\n",
            "      BoxPredictor_4/ClassPredictor/biases (48, 48/48 params)\n",
            "      BoxPredictor_4/ClassPredictor/weights (3x3x256x48, 110.59k/110.59k params)\n",
            "  BoxPredictor_5 (--/83.02k params)\n",
            "    BoxPredictor_5/BoxEncodingPredictor (--/27.67k params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor/weights (3x3x128x24, 27.65k/27.65k params)\n",
            "    BoxPredictor_5/ClassPredictor (--/55.34k params)\n",
            "      BoxPredictor_5/ClassPredictor/biases (48, 48/48 params)\n",
            "      BoxPredictor_5/ClassPredictor/weights (3x3x128x48, 55.30k/55.30k params)\n",
            "  FeatureExtractor (--/2.84m params)\n",
            "    FeatureExtractor/MobilenetV2 (--/2.84m params)\n",
            "      FeatureExtractor/MobilenetV2/Conv (--/864 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv/weights (3x3x3x32, 864/864 params)\n",
            "      FeatureExtractor/MobilenetV2/Conv_1 (--/409.60k params)\n",
            "        FeatureExtractor/MobilenetV2/Conv_1/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv_1/weights (1x1x320x1280, 409.60k/409.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv (--/800 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv/depthwise (--/288 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights (3x3x32x1, 288/288 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv/project (--/512 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/project/weights (1x1x32x16, 512/512 params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_1 (--/4.70k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise (--/864 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights (3x3x96x1, 864/864 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/expand (--/1.54k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights (1x1x16x96, 1.54k/1.54k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/project (--/2.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights (1x1x96x24, 2.30k/2.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_10 (--/64.90k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/project (--/36.86k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights (1x1x384x96, 36.86k/36.86k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_11 (--/115.78k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/project (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_12 (--/115.78k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/project (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_13 (--/152.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/project (--/92.16k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights (1x1x576x160, 92.16k/92.16k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_14 (--/315.84k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/project (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights (1x1x960x160, 153.60k/153.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_15 (--/315.84k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/project (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights (1x1x960x160, 153.60k/153.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_16 (--/469.44k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/project (--/307.20k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights (1x1x960x320, 307.20k/307.20k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_2 (--/8.21k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise (--/1.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/expand (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights (1x1x24x144, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/project (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights (1x1x144x24, 3.46k/3.46k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_3 (--/9.36k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise (--/1.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/expand (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights (1x1x24x144, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/project (--/4.61k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights (1x1x144x32, 4.61k/4.61k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_4 (--/14.02k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/project (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights (1x1x192x32, 6.14k/6.14k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_5 (--/14.02k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/project (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights (1x1x192x32, 6.14k/6.14k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_6 (--/20.16k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/project (--/12.29k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights (1x1x192x64, 12.29k/12.29k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_7 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_8 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_9 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256 (--/327.68k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights (1x1x1280x256, 327.68k/327.68k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128 (--/65.54k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights (1x1x512x128, 65.54k/65.54k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights (1x1x256x128, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64 (--/16.38k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights (1x1x256x64, 16.38k/16.38k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512 (--/131.07k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights (1x1x256x512, 131.07k/131.07k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise (--/2.30k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights (1x1x128x256, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise (--/1.15k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights (1x1x128x256, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise (--/1.15k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128 (--/8.19k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights (1x1x64x128, 8.19k/8.19k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise (--/576 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights (3x3x64x1, 576/576 params)\n",
            "\n",
            "======================End of Report==========================\n",
            "149 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              1\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   float_ops\n",
            "-account_type_regexes       .*\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*,.*Initializer.*,.*Regularizer.*,.*BiasAdd.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     float_ops\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
            "\n",
            "Profile:\n",
            "node name | # float_ops\n",
            "_TFProfRoot (--/13.72k flops)\n",
            "  MultipleGridAnchorGenerator/sub (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_20 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_19 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_28 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_27 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/sub_1 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_21 (1.08k/1.08k flops)\n",
            "  MultipleGridAnchorGenerator/mul_29 (600/600 flops)\n",
            "  MultipleGridAnchorGenerator/mul_35 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_36 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/sub_2 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_37 (150/150 flops)\n",
            "  MultipleGridAnchorGenerator/mul_43 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/sub_3 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_44 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_45 (54/54 flops)\n",
            "  MultipleGridAnchorGenerator/mul_51 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/sub_4 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_52 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_53 (24/24 flops)\n",
            "  MultipleGridAnchorGenerator/mul_17 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/mul_18 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/mul_60 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_59 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/sub_5 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_26 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/mul_25 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/mul_56 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_46 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_47 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_48 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_54 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_55 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_19 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_61 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_15 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_16 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_17 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_18 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_40 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_39 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_38 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_22 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_32 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_31 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_30 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_23 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_24 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_34 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/mul_33 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/mul_41 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_14 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_15 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_16 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_14 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_42 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_50 (2/2 flops)\n",
            "  MultipleGridAnchorGenerator/mul_49 (2/2 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_10 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_11 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_1 (1/1 flops)\n",
            "  Preprocessor/map/while/Less_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_12 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_13 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_19 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/Minimum (1/1 flops)\n",
            "  Preprocessor/map/while/Less (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/ones/Less (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_18 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_57 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_13 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_11 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_1 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_9 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_8 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_7 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_6 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_58 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_5 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_4 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_3 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_13 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_11 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_1 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/assert_equal_1/Equal (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_9 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_8 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_7 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_6 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_5 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_4 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_3 (1/1 flops)\n",
            "\n",
            "======================End of Report==========================\n",
            "2020-06-04 20:18:31.810629: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-06-04 20:18:31.868046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-04 20:18:31.868854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-06-04 20:18:31.869162: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-06-04 20:18:32.178147: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-06-04 20:18:32.343513: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-06-04 20:18:32.373126: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-06-04 20:18:32.710944: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-06-04 20:18:32.762600: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-06-04 20:18:33.322086: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-06-04 20:18:33.322377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-04 20:18:33.323226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-04 20:18:33.323941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2020-06-04 20:18:33.458789: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2020-06-04 20:18:33.461835: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2050a00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-06-04 20:18:33.461875: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-06-04 20:18:33.573594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-04 20:18:33.574437: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2050bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-06-04 20:18:33.574472: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2020-06-04 20:18:33.579145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-04 20:18:33.579889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-06-04 20:18:33.579988: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-06-04 20:18:33.580036: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-06-04 20:18:33.580072: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-06-04 20:18:33.580099: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-06-04 20:18:33.580126: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-06-04 20:18:33.580152: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-06-04 20:18:33.580180: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-06-04 20:18:33.580298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-04 20:18:33.581091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-04 20:18:33.581795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2020-06-04 20:18:33.586409: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-06-04 20:18:33.591968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-06-04 20:18:33.592013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2020-06-04 20:18:33.592047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2020-06-04 20:18:33.599771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-04 20:18:33.601230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-04 20:18:33.605556: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-06-04 20:18:33.605614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-25431\n",
            "I0604 20:18:33.609022 139959732991872 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-25431\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "W0604 20:18:42.018365 139959732991872 deprecation.py:323] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2020-06-04 20:18:42.957752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-04 20:18:42.958611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-06-04 20:18:42.958698: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-06-04 20:18:42.958756: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-06-04 20:18:42.958800: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-06-04 20:18:42.958849: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-06-04 20:18:42.958890: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-06-04 20:18:42.958932: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-06-04 20:18:42.958971: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-06-04 20:18:42.959110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-04 20:18:42.959891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-04 20:18:42.960605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2020-06-04 20:18:42.960657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-06-04 20:18:42.960679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2020-06-04 20:18:42.960694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2020-06-04 20:18:42.960863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-04 20:18:42.961641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-04 20:18:42.962354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-25431\n",
            "I0604 20:18:42.964550 139959732991872 saver.py:1284] Restoring parameters from drive/My Drive/training_logs_cls/model.ckpt-25431\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "W0604 20:18:43.729283 139959732991872 deprecation.py:323] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "W0604 20:18:43.729631 139959732991872 deprecation.py:323] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "INFO:tensorflow:Froze 344 variables.\n",
            "I0604 20:18:44.170168 139959732991872 graph_util_impl.py:334] Froze 344 variables.\n",
            "INFO:tensorflow:Converted 344 variables to const ops.\n",
            "I0604 20:18:44.272851 139959732991872 graph_util_impl.py:394] Converted 344 variables to const ops.\n",
            "2020-06-04 20:18:44.430133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-04 20:18:44.430908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-06-04 20:18:44.431003: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-06-04 20:18:44.431042: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-06-04 20:18:44.431086: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-06-04 20:18:44.431114: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-06-04 20:18:44.431141: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-06-04 20:18:44.431170: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-06-04 20:18:44.431200: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-06-04 20:18:44.431312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-04 20:18:44.432063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-04 20:18:44.432730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2020-06-04 20:18:44.432775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-06-04 20:18:44.432792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2020-06-04 20:18:44.432801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2020-06-04 20:18:44.432932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-04 20:18:44.433725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-06-04 20:18:44.434382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:326: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "W0604 20:18:44.993196 139959732991872 deprecation.py:323] From /content/models/research/object_detection/exporter.py:326: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "INFO:tensorflow:No assets to save.\n",
            "I0604 20:18:44.994092 139959732991872 builder_impl.py:640] No assets to save.\n",
            "INFO:tensorflow:No assets to write.\n",
            "I0604 20:18:44.994254 139959732991872 builder_impl.py:460] No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: trained_model/saved_model/saved_model.pb\n",
            "I0604 20:18:45.346039 139959732991872 builder_impl.py:425] SavedModel written to: trained_model/saved_model/saved_model.pb\n",
            "INFO:tensorflow:Writing pipeline config file to trained_model/pipeline.config\n",
            "I0604 20:18:45.378297 139959732991872 config_util.py:225] Writing pipeline config file to trained_model/pipeline.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b2nC6RWZcXvy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "1c3b359a-4701-4d72-e8a8-d1f91c5bf896"
      },
      "source": [
        "!wget https://github.com/tensorflow/models/raw/master/research/object_detection/export_tflite_ssd_graph.py\n",
        "!wget https://github.com/opencv/opencv/raw/master/samples/dnn/tf_text_graph_ssd.py\n",
        "!wget https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/tf_text_graph_common.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-04 20:19:03--  https://github.com/tensorflow/models/raw/master/research/object_detection/export_tflite_ssd_graph.py\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/export_tflite_ssd_graph.py [following]\n",
            "--2020-06-04 20:19:04--  https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/export_tflite_ssd_graph.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5894 (5.8K) [text/plain]\n",
            "Saving to: ‘export_tflite_ssd_graph.py’\n",
            "\n",
            "export_tflite_ssd_g 100%[===================>]   5.76K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-06-04 20:19:05 (34.3 MB/s) - ‘export_tflite_ssd_graph.py’ saved [5894/5894]\n",
            "\n",
            "--2020-06-04 20:19:07--  https://github.com/opencv/opencv/raw/master/samples/dnn/tf_text_graph_ssd.py\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/tf_text_graph_ssd.py [following]\n",
            "--2020-06-04 20:19:07--  https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/tf_text_graph_ssd.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17976 (18K) [text/plain]\n",
            "Saving to: ‘tf_text_graph_ssd.py’\n",
            "\n",
            "tf_text_graph_ssd.p 100%[===================>]  17.55K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-06-04 20:19:07 (1.76 MB/s) - ‘tf_text_graph_ssd.py’ saved [17976/17976]\n",
            "\n",
            "--2020-06-04 20:19:09--  https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/tf_text_graph_common.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9921 (9.7K) [text/plain]\n",
            "Saving to: ‘tf_text_graph_common.py’\n",
            "\n",
            "tf_text_graph_commo 100%[===================>]   9.69K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-06-04 20:19:09 (83.8 MB/s) - ‘tf_text_graph_common.py’ saved [9921/9921]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-NYPYlrpDKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content\n",
        "!mkdir tflite_out\n",
        "!python3 export_tflite_ssd_graph.py --pipeline_config_path=trained_model/pipeline.config --trained_checkpoint_prefix=trained_model/model.ckpt --output_directory=tflite_out/ --add_postprocessing_op=true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlSAExLJsBc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tflite_convert \\\n",
        "  --output_file=tflite_out/detect_garbage.tflite \\\n",
        "  --graph_def_file=tflite_out/tflite_graph.pb \\\n",
        "  --input_arrays=normalized_input_image_tensor \\\n",
        "  --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\n",
        "  --input_shape=1,300,300,3 \\\n",
        "  --inference_type=FLOAT \\\n",
        "  --allow_custom_ops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aisc0ImrgpBQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "71d3636c-438e-4cb4-ca6b-c0abbf489804"
      },
      "source": [
        "%cd /content\n",
        "!python3 tf_text_graph_ssd.py --input trained_model/frozen_inference_graph.pb --config trained_model/pipeline.config --output trained_model/graph.pbtxt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Scale: [0.200000-0.950000]\n",
            "Aspect ratios: [1.0, 2.0, 0.5, 3.0, 0.33329999]\n",
            "Reduce boxes in the lowest layer: True\n",
            "Number of classes: 7\n",
            "Number of layers: 6\n",
            "box predictor: convolutional\n",
            "Input image size: 300x300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ncBabYm5cCaK",
        "colab": {}
      },
      "source": [
        "# !zip -r trained_model.zip trained_model/\n",
        "# !cp -r /content/trained_model/ /content/drive/My\\ Drive/\n",
        "# !cp -r /content/tflite_out/ /content/drive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wbOv2yMnqlw6",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
        "PATH_TO_CKPT = \"/content/trained_model/frozen_inference_graph.pb\"\n",
        "\n",
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = \"/content/garbage_detection/label_map.pbtxt\"\n",
        "\n",
        "# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n",
        "PATH_TO_TEST_IMAGES_DIR =  \"/content/garbage_detection/data/garbage_dataset/\"\n",
        "\n",
        "# assert os.path.isfile(pb_fname)\n",
        "# assert os.path.isfile(PATH_TO_LABELS)\n",
        "# TEST_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, \"*.jpg\"))[-8:]\n",
        "# assert len(TEST_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_TEST_IMAGES_DIR)\n",
        "# print(TEST_IMAGE_PATHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyAjAPtNWIUA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c345d8d5-180b-462a-e362-56b07d4db1d1"
      },
      "source": [
        "TEST_IMAGE_PATHS = []\n",
        "with open(\"/content/garbage_detection/test_labels.csv\",'r') as f:\n",
        "  timgs=f.readlines()[50:70]\n",
        "  for i in timgs:\n",
        "    TEST_IMAGE_PATHS.append(PATH_TO_TEST_IMAGES_DIR+i.split(',')[0])\n",
        "print(TEST_IMAGE_PATHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/garbage_detection/data/garbage_dataset/batch_10/000036.jpg', '/content/garbage_detection/data/garbage_dataset/batch_7/000129.JPG', '/content/garbage_detection/data/garbage_dataset/batch_5/000030.JPG', '/content/garbage_detection/data/garbage_dataset/batch_12/000050.jpg', '/content/garbage_detection/data/garbage_dataset/batch_7/000065.JPG', '/content/garbage_detection/data/garbage_dataset/batch_5/000084.JPG', '/content/garbage_detection/data/garbage_dataset/batch_7/000080.JPG', '/content/garbage_detection/data/garbage_dataset/batch_11/000030.jpg', '/content/garbage_detection/data/garbage_dataset/batch_2/000082.JPG', '/content/garbage_detection/data/garbage_dataset/batch_12/000026.jpg', '/content/garbage_detection/data/garbage_dataset/batch_2/000013.JPG', '/content/garbage_detection/data/garbage_dataset/batch_13/000014.jpg', '/content/garbage_detection/data/garbage_dataset/batch_14/000045.jpg', '/content/garbage_detection/data/garbage_dataset/batch_13/000044.jpg', '/content/garbage_detection/data/garbage_dataset/batch_1/000076.JPG', '/content/garbage_detection/data/garbage_dataset/batch_1/000127.JPG', '/content/garbage_detection/data/garbage_dataset/batch_4/000005.JPG', '/content/garbage_detection/data/garbage_dataset/batch_10/000050.jpg', '/content/garbage_detection/data/garbage_dataset/batch_1/000025.jpg', '/content/garbage_detection/data/garbage_dataset/batch_12/000005.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h55xYwCwtmeM",
        "colab": {}
      },
      "source": [
        "%cd /content/models/research/object_detection\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "num_classes=7\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "\n",
        "# This is needed to display the images.\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(\n",
        "    label_map, max_num_classes=num_classes, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "\n",
        "def load_image_into_numpy_array(image):\n",
        "    (im_width, im_height) = image.size\n",
        "    return np.array(image.getdata()).reshape(\n",
        "        (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "# Size, in inches, of the output images.\n",
        "IMAGE_SIZE = (36, 24)\n",
        "\n",
        "\n",
        "def run_inference_for_single_image(image, graph):\n",
        "    with graph.as_default():\n",
        "        with tf.Session() as sess:\n",
        "            # Get handles to input and output tensors\n",
        "            ops = tf.get_default_graph().get_operations()\n",
        "            all_tensor_names = {\n",
        "                output.name for op in ops for output in op.outputs}\n",
        "            tensor_dict = {}\n",
        "            for key in [\n",
        "                'num_detections', 'detection_boxes', 'detection_scores',\n",
        "                'detection_classes', 'detection_masks'\n",
        "            ]:\n",
        "                tensor_name = key + ':0'\n",
        "                if tensor_name in all_tensor_names:\n",
        "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "                        tensor_name)\n",
        "            if 'detection_masks' in tensor_dict:\n",
        "                # The following processing is only for single image\n",
        "                detection_boxes = tf.squeeze(\n",
        "                    tensor_dict['detection_boxes'], [0])\n",
        "                detection_masks = tf.squeeze(\n",
        "                    tensor_dict['detection_masks'], [0])\n",
        "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "                real_num_detection = tf.cast(\n",
        "                    tensor_dict['num_detections'][0], tf.int32)\n",
        "                detection_boxes = tf.slice(detection_boxes, [0, 0], [\n",
        "                                           real_num_detection, -1])\n",
        "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n",
        "                                           real_num_detection, -1, -1])\n",
        "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
        "                detection_masks_reframed = tf.cast(\n",
        "                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "                # Follow the convention by adding back the batch dimension\n",
        "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
        "                    detection_masks_reframed, 0)\n",
        "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "            # Run inference\n",
        "            output_dict = sess.run(tensor_dict,\n",
        "                                   feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "\n",
        "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "            output_dict['num_detections'] = int(\n",
        "                output_dict['num_detections'][0])\n",
        "            output_dict['detection_classes'] = output_dict[\n",
        "                'detection_classes'][0].astype(np.uint8)\n",
        "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "            if 'detection_masks' in output_dict:\n",
        "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "    return output_dict\n",
        "\n",
        "\n",
        "for image_path in TEST_IMAGE_PATHS:\n",
        "    image = Image.open(image_path)\n",
        "    # the array based representation of the image will be used later in order to prepare the\n",
        "    # result image with boxes and labels on it.\n",
        "    image_np = load_image_into_numpy_array(image)\n",
        "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "    # Actual detection.\n",
        "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "    # Visualization of the results of a detection.\n",
        "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np,\n",
        "        output_dict['detection_boxes'],\n",
        "        output_dict['detection_classes'],\n",
        "        output_dict['detection_scores'],\n",
        "        category_index,\n",
        "        instance_masks=output_dict.get('detection_masks'),\n",
        "        use_normalized_coordinates=True,\n",
        "        line_thickness=6)\n",
        "    plt.figure(figsize=IMAGE_SIZE)\n",
        "    plt.imshow(image_np)\n",
        "    plt.imsave(\"/content/drive/My Drive/detect_out/\"+image_path.split('/')[-1],image_np)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C83h1DcqP_RW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}